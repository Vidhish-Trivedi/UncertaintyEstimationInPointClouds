{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMODIFIED ONE - CLONED FROM THE ORIGINAL CODE\\n\\nclass_label is still 9; Since we used the 10 class model\\n\\nIf you want to run, models separately search for \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MODIFIED ONE - CLONED FROM THE ORIGINAL CODE\n",
    "\n",
    "class_label is still 9; Since we used the 10 class model\n",
    "\n",
    "If you want to run, models separately search for \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Wait!!! Don't run the script right away..</span>\n",
    "If you want to run models separately search for #LOADPICKLESANDRUN (to save time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------<Import libraries>--------------||\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#SUPPRESS WARNINGS\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the filename : features_L003\n"
     ]
    }
   ],
   "source": [
    "fileName = input(\"Enter the filename :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Load computed features>--------||\n",
    "data = pd.read_csv(f\"ProcessedData/{fileName}.csv\")\n",
    "\n",
    "fileName = fileName.replace(\"features_\", \"\")\n",
    "\n",
    "data = data.drop(['Unnamed: 0'], axis=1)\n",
    "data = data.dropna()\n",
    "data = data.reset_index()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(float)\n",
    "grouped = data.groupby(data['label'])\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Check distribution>------------||\n",
    "averages = grouped.mean()\n",
    "variances = grouped.var()\n",
    "averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Run from here if the Kernel is getting trashed..</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#--------------------------------------------------------------------------------------------<Import libraries>--------------||\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#SUPPRESS WARNINGS\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "fileName = \"features_L003\"\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv(\"ProcessedData/features_L003.csv\")\n",
    "data = data.drop(['Unnamed: 0'], axis=1).dropna().reset_index()\n",
    "data = data.astype(float)\n",
    "\n",
    "# Group data\n",
    "grouped = data.groupby(data['label'])\n",
    "\n",
    "# Save and unload `averages`\n",
    "averages = grouped.mean()\n",
    "with open(\"tempDumps/averages.pkl\", \"wb\") as f:\n",
    "    pickle.dump(averages, f)\n",
    "del averages\n",
    "gc.collect()\n",
    "\n",
    "# Save and unload `variances`\n",
    "variances = grouped.var()\n",
    "with open(\"tempDumps/variances.pkl\", \"wb\") as f:\n",
    "    pickle.dump(variances, f)\n",
    "del variances\n",
    "gc.collect()\n",
    "\n",
    "# Save and unload `grouped`\n",
    "# Convert to dict-of-dfs before pickling\n",
    "grouped_dict = {k: v for k, v in grouped}\n",
    "with open(\"tempDumps/grouped.pkl\", \"wb\") as f:\n",
    "    pickle.dump(grouped_dict, f)\n",
    "del grouped\n",
    "del grouped_dict\n",
    "gc.collect()\n",
    "\"\"\"\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------<Compute cov matx>--------------||\n",
    "def compute_covariance_matrix(data, regularization=0):\n",
    "    cov_matrix = np.cov(data, rowvar=False)\n",
    "    cov_matrix += regularization * np.eye(cov_matrix.shape[0]) #Helps in computing Gaussian PDF\n",
    "    return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------<Get info for uncertainty est>--||\n",
    "def fit(x_train, y_train):\n",
    "    y_train = y_train.ravel()\n",
    "    m = y_train.shape[0] \n",
    "    x_train = x_train.reshape(m, -1)\n",
    "    input_feature = x_train.shape[1]\n",
    "    \n",
    "    #class_label = 9\n",
    "    class_label = 9 #Toronto\n",
    "\n",
    "    \"\"\"\n",
    "    mu    : (μ) Denotes the population mean (Class wise mean feature vector)\n",
    "    phi   : (Φ) Prior probability of each class\n",
    "    sigma : (Σ) Covariance matrix of each class\n",
    "    \"\"\"\n",
    "    \n",
    "    mu = np.zeros((class_label, input_feature))                    # Mean vectors per class\n",
    "    sigma = np.zeros((class_label, input_feature, input_feature))  # Covariance matrices per class\n",
    "    phi = np.zeros(class_label)                                    # Prior probability per class\n",
    "\n",
    "    for label in range(class_label):\n",
    "        indices = (y_train == label)\n",
    "        phi[label] = float(np.sum(indices)) / m\n",
    "        mu[label] = np.mean(x_train[indices, :], axis=0)\n",
    "        sigma[label] = compute_covariance_matrix(x_train[indices, :])\n",
    "    \n",
    "    return phi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['index', 'x', 'y', 'z', 'eigenvalue_sum', 'omnivariance',\n",
       "       'eigenentropy', 'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n",
       "       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz',\n",
       "       'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_nan = data.isnull().values.any()\n",
    "print(has_nan)\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">Normalization : Exlcuding z (temporary, the iidea is to consider x,y for training)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>eigenvalue_sum</th>\n",
       "      <th>omnivariance</th>\n",
       "      <th>eigenentropy</th>\n",
       "      <th>anisotropy</th>\n",
       "      <th>planarity</th>\n",
       "      <th>linearity</th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "      <th>surface_variation</th>\n",
       "      <th>sphericity</th>\n",
       "      <th>verticality</th>\n",
       "      <th>nx</th>\n",
       "      <th>ny</th>\n",
       "      <th>nz</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>173.641998</td>\n",
       "      <td>530.587006</td>\n",
       "      <td>141.451004</td>\n",
       "      <td>0.235233</td>\n",
       "      <td>7.521173e-02</td>\n",
       "      <td>0.311788</td>\n",
       "      <td>0.990949</td>\n",
       "      <td>9.655324e-02</td>\n",
       "      <td>0.894694</td>\n",
       "      <td>0.845515</td>\n",
       "      <td>1.890141e-01</td>\n",
       "      <td>2.393549e-02</td>\n",
       "      <td>9.050911e-03</td>\n",
       "      <td>0.765975</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.554805</td>\n",
       "      <td>0.382989</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>203.190002</td>\n",
       "      <td>539.214996</td>\n",
       "      <td>138.509995</td>\n",
       "      <td>0.173790</td>\n",
       "      <td>5.505079e-02</td>\n",
       "      <td>0.252097</td>\n",
       "      <td>0.994291</td>\n",
       "      <td>2.058872e-01</td>\n",
       "      <td>0.788615</td>\n",
       "      <td>0.731055</td>\n",
       "      <td>3.473681e-01</td>\n",
       "      <td>1.382286e-02</td>\n",
       "      <td>5.709175e-03</td>\n",
       "      <td>0.853330</td>\n",
       "      <td>0.011531</td>\n",
       "      <td>0.422408</td>\n",
       "      <td>0.573337</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>197.794006</td>\n",
       "      <td>537.670990</td>\n",
       "      <td>137.229004</td>\n",
       "      <td>0.146784</td>\n",
       "      <td>1.880067e-11</td>\n",
       "      <td>0.199239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.725855e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.681306e-16</td>\n",
       "      <td>3.394411e-16</td>\n",
       "      <td>1.152201e-16</td>\n",
       "      <td>0.019736</td>\n",
       "      <td>0.768203</td>\n",
       "      <td>0.078542</td>\n",
       "      <td>0.521009</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>203.234985</td>\n",
       "      <td>539.347992</td>\n",
       "      <td>136.787994</td>\n",
       "      <td>0.323704</td>\n",
       "      <td>1.136059e-01</td>\n",
       "      <td>0.393411</td>\n",
       "      <td>0.979793</td>\n",
       "      <td>3.663016e-02</td>\n",
       "      <td>0.943800</td>\n",
       "      <td>0.893729</td>\n",
       "      <td>1.044633e-01</td>\n",
       "      <td>5.534011e-02</td>\n",
       "      <td>2.020722e-02</td>\n",
       "      <td>0.587015</td>\n",
       "      <td>0.321725</td>\n",
       "      <td>0.919021</td>\n",
       "      <td>0.293509</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>203.886993</td>\n",
       "      <td>539.648987</td>\n",
       "      <td>136.639008</td>\n",
       "      <td>0.117842</td>\n",
       "      <td>4.705418e-07</td>\n",
       "      <td>0.175056</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.321787e-02</td>\n",
       "      <td>0.936792</td>\n",
       "      <td>0.910308</td>\n",
       "      <td>1.188763e-01</td>\n",
       "      <td>7.059299e-17</td>\n",
       "      <td>2.547577e-17</td>\n",
       "      <td>0.038317</td>\n",
       "      <td>0.634707</td>\n",
       "      <td>0.525417</td>\n",
       "      <td>0.980845</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39688167</th>\n",
       "      <td>39721585.0</td>\n",
       "      <td>318.395020</td>\n",
       "      <td>368.444000</td>\n",
       "      <td>135.009003</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>2.645727e-03</td>\n",
       "      <td>0.276031</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.573247e-02</td>\n",
       "      <td>0.984268</td>\n",
       "      <td>0.976629</td>\n",
       "      <td>3.097030e-02</td>\n",
       "      <td>7.298831e-06</td>\n",
       "      <td>2.516479e-06</td>\n",
       "      <td>0.313917</td>\n",
       "      <td>0.638413</td>\n",
       "      <td>0.836399</td>\n",
       "      <td>0.843045</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39688168</th>\n",
       "      <td>39721586.0</td>\n",
       "      <td>317.992981</td>\n",
       "      <td>369.471985</td>\n",
       "      <td>135.011002</td>\n",
       "      <td>0.493942</td>\n",
       "      <td>4.284744e-01</td>\n",
       "      <td>0.619792</td>\n",
       "      <td>0.885199</td>\n",
       "      <td>4.390073e-01</td>\n",
       "      <td>0.449849</td>\n",
       "      <td>0.399281</td>\n",
       "      <td>6.622986e-01</td>\n",
       "      <td>2.036222e-01</td>\n",
       "      <td>1.148008e-01</td>\n",
       "      <td>0.848115</td>\n",
       "      <td>0.029956</td>\n",
       "      <td>0.347385</td>\n",
       "      <td>0.424059</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39688169</th>\n",
       "      <td>39721587.0</td>\n",
       "      <td>318.151978</td>\n",
       "      <td>369.438995</td>\n",
       "      <td>135.003998</td>\n",
       "      <td>0.340370</td>\n",
       "      <td>3.741837e-01</td>\n",
       "      <td>0.479280</td>\n",
       "      <td>0.653463</td>\n",
       "      <td>5.478430e-01</td>\n",
       "      <td>0.116534</td>\n",
       "      <td>0.170892</td>\n",
       "      <td>7.962999e-01</td>\n",
       "      <td>4.601972e-01</td>\n",
       "      <td>3.465368e-01</td>\n",
       "      <td>0.507828</td>\n",
       "      <td>0.933824</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>0.746089</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39688170</th>\n",
       "      <td>39721588.0</td>\n",
       "      <td>318.190979</td>\n",
       "      <td>369.076019</td>\n",
       "      <td>135.005997</td>\n",
       "      <td>0.423745</td>\n",
       "      <td>3.882006e-01</td>\n",
       "      <td>0.553028</td>\n",
       "      <td>0.857688</td>\n",
       "      <td>3.726005e-01</td>\n",
       "      <td>0.489594</td>\n",
       "      <td>0.406498</td>\n",
       "      <td>6.193314e-01</td>\n",
       "      <td>2.544225e-01</td>\n",
       "      <td>1.423117e-01</td>\n",
       "      <td>0.923139</td>\n",
       "      <td>0.082672</td>\n",
       "      <td>0.227309</td>\n",
       "      <td>0.461571</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39688171</th>\n",
       "      <td>39721589.0</td>\n",
       "      <td>318.213989</td>\n",
       "      <td>369.045013</td>\n",
       "      <td>135.031998</td>\n",
       "      <td>0.413927</td>\n",
       "      <td>3.734235e-01</td>\n",
       "      <td>0.537062</td>\n",
       "      <td>0.845322</td>\n",
       "      <td>1.646424e-01</td>\n",
       "      <td>0.685537</td>\n",
       "      <td>0.521538</td>\n",
       "      <td>4.294882e-01</td>\n",
       "      <td>3.112576e-01</td>\n",
       "      <td>1.546784e-01</td>\n",
       "      <td>0.778441</td>\n",
       "      <td>0.121157</td>\n",
       "      <td>0.193071</td>\n",
       "      <td>0.389222</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39688172 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               index           x           y           z  eigenvalue_sum  \\\n",
       "0                0.0  173.641998  530.587006  141.451004        0.235233   \n",
       "1                1.0  203.190002  539.214996  138.509995        0.173790   \n",
       "2                2.0  197.794006  537.670990  137.229004        0.146784   \n",
       "3                5.0  203.234985  539.347992  136.787994        0.323704   \n",
       "4                6.0  203.886993  539.648987  136.639008        0.117842   \n",
       "...              ...         ...         ...         ...             ...   \n",
       "39688167  39721585.0  318.395020  368.444000  135.009003        0.217842   \n",
       "39688168  39721586.0  317.992981  369.471985  135.011002        0.493942   \n",
       "39688169  39721587.0  318.151978  369.438995  135.003998        0.340370   \n",
       "39688170  39721588.0  318.190979  369.076019  135.005997        0.423745   \n",
       "39688171  39721589.0  318.213989  369.045013  135.031998        0.413927   \n",
       "\n",
       "          omnivariance  eigenentropy  anisotropy     planarity  linearity  \\\n",
       "0         7.521173e-02      0.311788    0.990949  9.655324e-02   0.894694   \n",
       "1         5.505079e-02      0.252097    0.994291  2.058872e-01   0.788615   \n",
       "2         1.880067e-11      0.199239    1.000000  2.725855e-16   1.000000   \n",
       "3         1.136059e-01      0.393411    0.979793  3.663016e-02   0.943800   \n",
       "4         4.705418e-07      0.175056    1.000000  6.321787e-02   0.936792   \n",
       "...                ...           ...         ...           ...        ...   \n",
       "39688167  2.645727e-03      0.276031    0.999997  1.573247e-02   0.984268   \n",
       "39688168  4.284744e-01      0.619792    0.885199  4.390073e-01   0.449849   \n",
       "39688169  3.741837e-01      0.479280    0.653463  5.478430e-01   0.116534   \n",
       "39688170  3.882006e-01      0.553028    0.857688  3.726005e-01   0.489594   \n",
       "39688171  3.734235e-01      0.537062    0.845322  1.646424e-01   0.685537   \n",
       "\n",
       "              PCA1          PCA2  surface_variation    sphericity  \\\n",
       "0         0.845515  1.890141e-01       2.393549e-02  9.050911e-03   \n",
       "1         0.731055  3.473681e-01       1.382286e-02  5.709175e-03   \n",
       "2         1.000000  7.681306e-16       3.394411e-16  1.152201e-16   \n",
       "3         0.893729  1.044633e-01       5.534011e-02  2.020722e-02   \n",
       "4         0.910308  1.188763e-01       7.059299e-17  2.547577e-17   \n",
       "...            ...           ...                ...           ...   \n",
       "39688167  0.976629  3.097030e-02       7.298831e-06  2.516479e-06   \n",
       "39688168  0.399281  6.622986e-01       2.036222e-01  1.148008e-01   \n",
       "39688169  0.170892  7.962999e-01       4.601972e-01  3.465368e-01   \n",
       "39688170  0.406498  6.193314e-01       2.544225e-01  1.423117e-01   \n",
       "39688171  0.521538  4.294882e-01       3.112576e-01  1.546784e-01   \n",
       "\n",
       "          verticality        nx        ny        nz  label  \n",
       "0            0.765975  0.016984  0.554805  0.382989    1.0  \n",
       "1            0.853330  0.011531  0.422408  0.573337    1.0  \n",
       "2            0.019736  0.768203  0.078542  0.521009    1.0  \n",
       "3            0.587015  0.321725  0.919021  0.293509    1.0  \n",
       "4            0.038317  0.634707  0.525417  0.980845    1.0  \n",
       "...               ...       ...       ...       ...    ...  \n",
       "39688167     0.313917  0.638413  0.836399  0.843045    7.0  \n",
       "39688168     0.848115  0.029956  0.347385  0.424059    7.0  \n",
       "39688169     0.507828  0.933824  0.535211  0.746089    7.0  \n",
       "39688170     0.923139  0.082672  0.227309  0.461571    7.0  \n",
       "39688171     0.778441  0.121157  0.193071  0.389222    7.0  \n",
       "\n",
       "[39688172 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<Nomralization>-----------------||\n",
    "from sklearn.preprocessing import MinMaxScaler #scikit-learn\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\"\"\"columns_to_scale = ['z', 'eigenvalue_sum', 'omnivariance', 'eigenentropy',\n",
    "       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n",
    "       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz']\"\"\" #Toronto\n",
    "\n",
    "columns_to_scale = ['eigenvalue_sum', 'omnivariance', 'eigenentropy',\n",
    "       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n",
    "       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz'] #Toronto, excluded z\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=columns_to_scale)\n",
    "data[columns_to_scale] = scaled_df\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#FIX Memory issues\n",
    "del scaled_df\n",
    "gc.collect()\n",
    "\n",
    "del scaler\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------<Prepare data>------------------||\n",
    "data = data.dropna()\n",
    "\n",
    "# x = data[['Column1','Column2','Column3','Column4','Column5','Column6','Column7','Column8']]\n",
    "\"\"\"X = data[['z', 'eigenvalue_sum', 'omnivariance', 'eigenentropy',\n",
    "       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n",
    "       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz']]\"\"\" #Toronto\n",
    "\n",
    "X = data[['x', 'y', 'z', 'eigenvalue_sum', 'omnivariance', 'eigenentropy',\n",
    "       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n",
    "       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz']] #Toronto\n",
    "\n",
    "y = data[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------<Data split>--------------------||\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #REVERTTHIS\n",
    "\n",
    "X_train_values = X_train.values\n",
    "y_train_values = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.64 s, sys: 2.03 s, total: 7.67 s\n",
      "Wall time: 7.68 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.0339873 , 0.51804724, 0.01977787, 0.04886462, 0.29416397,\n",
       "       0.0085094 , 0.01027847, 0.05971839, 0.00665274])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "phi, mu, sigma = fit(X_train_values, y_train_values)\n",
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tempDumps/phi.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_train, f)\n",
    "\n",
    "with open(\"tempDumps/mu.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_test, f)\n",
    "\n",
    "with open(\"tempDumps/sigma.pkl\", \"wb\") as f:\n",
    "    pickle.dump(y_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------<Multivariate gaussian pdf>-----||\n",
    "import math\n",
    "def multivariate_gaussian_pdf(x, mean, cov):\n",
    "    d = mean.shape[0] #dimensionality of the input\n",
    "    exponent = -0.5 * np.dot(np.dot((x - mean).T, np.linalg.inv(cov)), (x - mean)) # -(1/2) . Transpose(x−μ) . Inverse(Σ or Covariance Matx) . (x−μ)\n",
    "    prefactor = 1 / np.sqrt(((2 * np.pi) ** d )*(np.linalg.det(cov))) # 1 / Sqrt( (2π)^d . |Σ| ) \n",
    "    return np.exp(exponent)*prefactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.55248714e+03 3.98182086e+01 1.16161045e-01 1.62610609e-02\n",
      " 5.40388023e-03 2.58872940e-03 1.93247375e-03 1.38408743e-03\n",
      " 7.20984708e-04 4.45141017e-04 4.26351627e-04 2.56531030e-04\n",
      " 8.72992660e-06 1.65604279e-06 2.68466563e-15 2.75977172e-16\n",
      " 9.72418628e-16]\n",
      "The matrix is positive semidefinite.\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<Positive semi definite chck>---||\n",
    "# Covariance matrices must be positive semidefinite because variance (and covariance) can't be negative\n",
    "def is_positive_semidefinite(matrix):\n",
    "    eigenvalues, _ = np.linalg.eig(matrix)\n",
    "    print(eigenvalues)\n",
    "    return np.all(eigenvalues >= 0)\n",
    "\n",
    "matrix = sigma[1] \n",
    "# print(matrix)\n",
    "positive_semidefinite = is_positive_semidefinite(matrix)\n",
    "if positive_semidefinite:\n",
    "    print(\"The matrix is positive semidefinite.\")\n",
    "else:\n",
    "    print(\"The matrix is not positive semidefinite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.27333666e+02, -9.49835319e+01, -4.60377670e-01, ...,\n",
       "         -1.80438301e-01, -7.11182545e-02,  4.01884710e-01],\n",
       "        [-9.49835319e+01,  5.62381025e+02, -3.63344901e-01, ...,\n",
       "         -2.42029964e-01, -7.91947933e-02,  9.72890232e-01],\n",
       "        [-4.60377670e-01, -3.63344901e-01,  7.38963810e-01, ...,\n",
       "         -3.43020318e-02, -5.28816513e-03,  9.27511247e-02],\n",
       "        ...,\n",
       "        [-1.80438301e-01, -2.42029964e-01, -3.43020318e-02, ...,\n",
       "          3.36640994e-02,  8.29716812e-03, -2.13244353e-02],\n",
       "        [-7.11182545e-02, -7.91947933e-02, -5.28816513e-03, ...,\n",
       "          8.29716812e-03,  1.48109066e-02, -1.99740747e-03],\n",
       "        [ 4.01884710e-01,  9.72890232e-01,  9.27511247e-02, ...,\n",
       "         -2.13244353e-02, -1.99740747e-03,  5.25657624e-02]],\n",
       "\n",
       "       [[ 1.73056039e+02, -4.28729914e+02,  1.52235736e+00, ...,\n",
       "          4.00207899e-02, -1.45863353e-03,  3.81102552e-03],\n",
       "        [-4.28729914e+02,  1.41922245e+03, -4.21199982e+00, ...,\n",
       "          1.17621215e-02,  3.71686672e-02, -1.44566823e-02],\n",
       "        [ 1.52235736e+00, -4.21199982e+00,  3.03669999e-02, ...,\n",
       "         -2.35934011e-04, -1.10143010e-04,  7.08607417e-04],\n",
       "        ...,\n",
       "        [ 4.00207899e-02,  1.17621215e-02, -2.35934011e-04, ...,\n",
       "          1.61142588e-03,  3.52817609e-04, -2.98546328e-04],\n",
       "        [-1.45863353e-03,  3.71686672e-02, -1.10143010e-04, ...,\n",
       "          3.52817609e-04,  3.62617133e-04,  2.99496650e-06],\n",
       "        [ 3.81102552e-03, -1.44566823e-02,  7.08607417e-04, ...,\n",
       "         -2.98546328e-04,  2.99496650e-06,  1.50535209e-03]],\n",
       "\n",
       "       [[ 1.18725640e+02, -3.60281856e+02,  1.18105569e+00, ...,\n",
       "         -9.03006602e-03, -1.47698376e-02, -6.53889619e-04],\n",
       "        [-3.60281856e+02,  1.15832582e+03, -3.77192527e+00, ...,\n",
       "         -1.05389390e-02,  3.55342456e-02,  1.09446427e-03],\n",
       "        [ 1.18105569e+00, -3.77192527e+00,  1.53796318e-02, ...,\n",
       "         -1.17016694e-04, -9.69742472e-05, -1.94466026e-06],\n",
       "        ...,\n",
       "        [-9.03006602e-03, -1.05389390e-02, -1.17016694e-04, ...,\n",
       "          1.23026911e-04,  3.44988172e-05,  6.57893235e-07],\n",
       "        [-1.47698376e-02,  3.55342456e-02, -9.69742472e-05, ...,\n",
       "          3.44988172e-05,  4.44071309e-05,  1.27389853e-06],\n",
       "        [-6.53889619e-04,  1.09446427e-03, -1.94466026e-06, ...,\n",
       "          6.57893235e-07,  1.27389853e-06,  4.44569813e-06]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.51094397e+02, -6.40889784e+02,  1.16558295e+01, ...,\n",
       "         -8.04974012e-01,  1.09308267e+00, -7.36924979e-03],\n",
       "        [-6.40889784e+02,  2.01756198e+03, -2.16406416e+01, ...,\n",
       "         -3.70083738e-01, -1.01437191e+00,  1.51161331e-01],\n",
       "        [ 1.16558295e+01, -2.16406416e+01,  1.01888600e+01, ...,\n",
       "          1.01956338e-01,  1.11813740e-01, -6.98760053e-02],\n",
       "        ...,\n",
       "        [-8.04974012e-01, -3.70083738e-01,  1.01956338e-01, ...,\n",
       "          8.12320281e-02,  1.26126683e-02,  3.68361936e-05],\n",
       "        [ 1.09308267e+00, -1.01437191e+00,  1.11813740e-01, ...,\n",
       "          1.26126683e-02,  1.06547599e-01, -6.10223764e-03],\n",
       "        [-7.36924979e-03,  1.51161331e-01, -6.98760053e-02, ...,\n",
       "          3.68361936e-05, -6.10223764e-03,  1.97019023e-02]],\n",
       "\n",
       "       [[ 1.35817635e+02, -2.98949755e+02,  1.19719208e+00, ...,\n",
       "         -1.34128440e-01, -1.87783492e-02,  2.81591574e-02],\n",
       "        [-2.98949755e+02,  1.02695011e+03, -2.01008966e+00, ...,\n",
       "          6.36667763e-02,  7.68222502e-02,  2.81786153e-02],\n",
       "        [ 1.19719208e+00, -2.01008966e+00,  2.69984996e-01, ...,\n",
       "          1.26541929e-02,  2.63655213e-03, -6.86630841e-03],\n",
       "        ...,\n",
       "        [-1.34128440e-01,  6.36667763e-02,  1.26541929e-02, ...,\n",
       "          1.45435622e-01,  3.76670856e-02,  8.62226900e-03],\n",
       "        [-1.87783492e-02,  7.68222502e-02,  2.63655213e-03, ...,\n",
       "          3.76670856e-02,  3.09057578e-02,  3.73523628e-03],\n",
       "        [ 2.81591574e-02,  2.81786153e-02, -6.86630841e-03, ...,\n",
       "          8.62226900e-03,  3.73523628e-03,  5.92255812e-02]],\n",
       "\n",
       "       [[ 1.30506358e+02, -1.16592188e+02, -3.09579609e+00, ...,\n",
       "         -3.75379952e-01, -3.64877779e-01,  9.17589980e-02],\n",
       "        [-1.16592188e+02,  1.98072948e+02, -1.21099816e+00, ...,\n",
       "          7.69470159e-01,  6.83278308e-01, -1.58731934e-01],\n",
       "        [-3.09579609e+00, -1.21099816e+00,  7.55958234e-01, ...,\n",
       "         -1.39812858e-02, -1.90254268e-03,  3.26866772e-03],\n",
       "        ...,\n",
       "        [-3.75379952e-01,  7.69470159e-01, -1.39812858e-02, ...,\n",
       "          1.08947070e-01,  3.29190767e-02, -1.16871268e-04],\n",
       "        [-3.64877779e-01,  6.83278308e-01, -1.90254268e-03, ...,\n",
       "          3.29190767e-02,  2.53408700e-02,  5.54756521e-04],\n",
       "        [ 9.17589980e-02, -1.58731934e-01,  3.26866772e-03, ...,\n",
       "         -1.16871268e-04,  5.54756521e-04,  5.19305056e-03]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.82221154e+02 1.07518204e+02 8.04797633e-01 1.72178450e-01\n",
      " 9.25227568e-02 2.86445986e-02 1.81019275e-02 1.15488047e-02\n",
      " 7.88748214e-03 6.76031078e-03 1.08929104e-03 7.31285682e-04\n",
      " 1.25712376e-04 7.91154793e-06 2.37000048e-15 9.00276793e-16\n",
      " 3.06220459e-16]\n",
      "[1.55248714e+03 3.98182086e+01 1.16161045e-01 1.62610609e-02\n",
      " 5.40388023e-03 2.58872940e-03 1.93247375e-03 1.38408743e-03\n",
      " 7.20984708e-04 4.45141017e-04 4.26351627e-04 2.56531030e-04\n",
      " 8.72992660e-06 1.65604279e-06 2.68466563e-15 2.75977172e-16\n",
      " 9.72418628e-16]\n",
      "[1.27098991e+03 6.07565696e+00 8.70284846e-02 3.01166612e-03\n",
      " 1.91112449e-03 5.83794842e-04 2.41534141e-04 8.12588138e-05\n",
      " 2.18155983e-05 1.85888634e-05 5.44181113e-06 1.05354714e-06\n",
      " 9.49514545e-07 6.25522186e-08 2.58714618e-15 9.35455223e-16\n",
      " 2.06244085e-16]\n",
      "[2.30939461e+03 5.45245386e+02 9.52468758e+00 1.81765231e-01\n",
      " 1.31069491e-01 7.90433748e-02 6.08193641e-02 5.13894597e-02\n",
      " 4.04786324e-02 3.64884645e-02 2.25324804e-03 9.13781994e-04\n",
      " 2.75004063e-04 5.41761411e-05 2.11365334e-15 2.63038208e-16\n",
      " 7.06077053e-16]\n",
      "[7.59090005e+02 2.27231785e+02 7.68334041e+00 1.94155643e-01\n",
      " 1.13754458e-01 5.54628696e-02 3.37685913e-02 2.71141929e-02\n",
      " 1.34415085e-02 1.20732081e-02 1.69335678e-03 9.38451096e-04\n",
      " 1.34463316e-04 1.92873916e-05 3.23746995e-16 2.03883566e-15\n",
      " 8.25822965e-16]\n",
      "[1.44406488e+03 1.05859197e+02 3.50854695e+00 1.43994487e-01\n",
      " 9.57232539e-02 6.58035611e-02 3.32911778e-02 2.24166157e-02\n",
      " 1.29072718e-02 8.99604867e-03 9.17052533e-04 4.77606168e-04\n",
      " 7.32943147e-05 4.78760511e-05 7.91145530e-16 2.29843478e-15\n",
      " 2.29671029e-16]\n",
      "[1.11795234e+03 4.48455135e+01 2.96194612e-01 1.64647993e-01\n",
      " 1.46455012e-01 1.23328001e-01 9.85529244e-02 1.91750506e-02\n",
      " 1.59034480e-02 8.65894679e-03 2.70472397e-03 8.67867765e-04\n",
      " 3.36268294e-04 9.66492518e-06 2.14898065e-15 2.02033543e-16\n",
      " 7.27136764e-16]\n",
      "[2.85699766e+02 4.31537637e+01 5.12402443e-01 1.22130158e-01\n",
      " 1.00590389e-01 1.99334400e-02 1.24948476e-02 8.63583961e-03\n",
      " 4.30637533e-03 2.59068991e-03 7.72887049e-04 7.01080288e-04\n",
      " 4.54324540e-05 3.80607689e-06 2.39457381e-15 2.63724665e-16\n",
      " 8.09541747e-16]\n"
     ]
    }
   ],
   "source": [
    "for label in (0,1,2,3,4,5,7,8): #Toronto\n",
    "    print(np.linalg.eigvals(sigma[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Epistemic uncertainty calculation</span>\n",
    "Computes the epistemic uncertainty using this formula 1 − ∑ P( y=c | x )⋅ϕ(c)\n",
    "- P( y=c | x ) is class probability from the Gaussian\n",
    "- ϕ(c) is the prior (Φ, Prior probability of each class or Class prior probabilities (class frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------<Compute epistemic uncertnty>---||\n",
    "def give_epistemic(X_test):\n",
    "    x_test = X_test.values\n",
    "    feature_densities = []\n",
    "    for i in range (x_test.shape[0]):\n",
    "        rel_probs = []\n",
    "        deno = 0\n",
    "        \n",
    "        #for label in (1,2,5,8):\n",
    "        for label in (0,1,2,3,4,5,6,7,8): #Toronto\n",
    "            x = multivariate_gaussian_pdf(x_test[i], mu[label], sigma[label])\n",
    "            deno += x\n",
    "            rel_probs.append(x)\n",
    "        probs = [x/deno for x in rel_probs]\n",
    "        feature_density = 0\n",
    "        \n",
    "        #labels = [1,2,5,8]\n",
    "        labels = [0,1,2,3,4,5,6,7,8] #Toronto\n",
    "        for j in range (len(labels)):\n",
    "            feature_density += phi[labels[j]]*probs[j]\n",
    "        feature_densities.append([x_test[i], feature_density])\n",
    "    \n",
    "    epistemic_uncertainty = []\n",
    "    for i in feature_densities:\n",
    "        epistemic_uncertainty.append(1-i[1])\n",
    "    return epistemic_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Aleatoric uncertainty calculation</span>\n",
    "Computes the aleatoric uncertainty using this formula Entropy(x) = −∑p_i ⋅ log(p_i). Uncertainty from softmax entropy (i.e., model's own prediction)\n",
    "- Computes entropy of the softmax output for each point\n",
    "- Higher entropy = more aleatoric uncertainty (the model is confused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------<Compute aleatoric uncertnty>---||\n",
    "def get_aleatoric(X_test, softmax_probs):\n",
    "    entropies = []\n",
    "    sum_probs = []\n",
    "    for i in range (len(softmax_probs)):\n",
    "        # sum_prob = 0\n",
    "        for j in softmax_probs[i]:\n",
    "            entropy = 0\n",
    "            if (j == 0):\n",
    "                continue\n",
    "            else:\n",
    "                entropy+= -j*np.log(j)\n",
    "            # sum_prob += j\n",
    "            # sum_probs.append(sum_prob)   \n",
    "        entropies.append(entropy)\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 16min 46s, sys: 45.2 s, total: 1h 17min 31s\n",
      "Wall time: 1h 17min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Compute epistemic\n",
    "X_epistemic = give_epistemic(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['epistemic'] = X_epistemic\n",
    "data_new = pd.concat([X, y], axis=1)\n",
    "data_new = data_new.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ProcessedData/UncertaintyAdded/fullFeatures_L003.csv\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<To CSV>------------------------||\n",
    "import pickle\n",
    "\n",
    "outputPath = f\"ProcessedData/UncertaintyAdded/fullFeatures_{fileName}.csv\"\n",
    "data_new.to_csv(outputPath, index=False)\n",
    "print(f\"Data saved to {outputPath}\")\n",
    "\n",
    "with open(\"TrainTestSplitPkl/fileName.pkl\", \"wb\") as f:\n",
    "    pickle.dump(fileName, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Load from here !!! Uncertainty estimated</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#--------------------------------------------------------------------------------------------<Get CSV>-----------------------||\\n\\nimport math\\nimport numpy as np\\nimport pandas as pd\\nimport os\\nimport warnings\\nimport pickle\\n\\nfrom sklearn.model_selection import train_test_split\\nwarnings.filterwarnings(\"ignore\")\\n\\nwith open(\"TrainTestSplitPkl/fileName.pkl\", \"rb\") as f:\\n        fileName = pickle.load(f)\\n\\ndata_new = pd.read_csv(f\"ProcessedData/UncertaintyAdded/fullFeatures_ParisLilleFullMergedData.csv\")\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#--------------------------------------------------------------------------------------------<Get CSV>-----------------------||\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with open(\"TrainTestSplitPkl/fileName.pkl\", \"rb\") as f:\n",
    "        fileName = pickle.load(f)\n",
    "\n",
    "data_new = pd.read_csv(f\"ProcessedData/UncertaintyAdded/fullFeatures_ParisLilleFullMergedData.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------<Prepare data>------------------||\n",
    "\"\"\"X_new = data_new[['z', 'eigenvalue_sum', 'omnivariance', 'eigenentropy',\n",
    "       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n",
    "       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz', 'epistemic']]\"\"\"\n",
    "\n",
    "X_new = data_new[['x', 'y', 'z', 'eigenvalue_sum', 'omnivariance', 'eigenentropy',\n",
    "       'anisotropy', 'planarity', 'linearity', 'PCA1', 'PCA2',\n",
    "       'surface_variation', 'sphericity', 'verticality', 'nx', 'ny', 'nz', 'epistemic']] #Toronto, 18 including x,y\n",
    "\n",
    "y_new = data_new[['label']]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=45)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2) #REVERTTHIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\n\\ndel X_new\\ngc.collect()\\n\\ndel y_new\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "\n",
    "del X_new\n",
    "gc.collect()\n",
    "\n",
    "del y_new\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the data as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pkl files to disk.\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<Save the data>-----------------||\n",
    "import pickle\n",
    "\n",
    "with open(\"TrainTestSplitPkl/X_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_train, f)\n",
    "\n",
    "with open(\"TrainTestSplitPkl/X_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_test, f)\n",
    "\n",
    "with open(\"TrainTestSplitPkl/y_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(y_train, f)\n",
    "\n",
    "with open(\"TrainTestSplitPkl/y_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(y_test, f)\n",
    "\n",
    "with open(\"TrainTestSplitPkl/fileName.pkl\", \"wb\") as f:\n",
    "    pickle.dump(fileName, f)\n",
    "\n",
    "print(\"Saved pkl files to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pickle files\n",
    "- Optional only (LOAD from here to continue with the work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------<Load the pickle file>----------||\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#SUPPRESS WARNINGS\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train, X_test, y_train, y_test, fileName = None, None, None, None, None\n",
    "\n",
    "#LOADPICKLESANDRUN\n",
    "def loadPkl():\n",
    "    with open(\"TrainTestSplitPkl/X_train.pkl\", \"rb\") as f:\n",
    "        X_train = pickle.load(f)\n",
    "    \n",
    "    with open(\"TrainTestSplitPkl/X_test.pkl\", \"rb\") as f:\n",
    "        X_test = pickle.load(f)\n",
    "    \n",
    "    with open(\"TrainTestSplitPkl/y_train.pkl\", \"rb\") as f:\n",
    "        y_train = pickle.load(f)\n",
    "    \n",
    "    with open(\"TrainTestSplitPkl/y_test.pkl\", \"rb\") as f:\n",
    "        y_test = pickle.load(f)\n",
    "    \n",
    "    with open(\"TrainTestSplitPkl/fileName.pkl\", \"rb\") as f:\n",
    "        fileName = pickle.load(f)\n",
    "    \n",
    "    print(\"Loaded pkl files from disk.\")\n",
    "    return X_train, X_test, y_train, y_test, fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pkl files from disk.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, fileName = loadPkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_L003\n"
     ]
    }
   ],
   "source": [
    "print(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:yellow\">1. Model with uncertainty as a feature</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          label\n",
      "31308525    4.0\n",
      "34079256    4.0\n",
      "11043232    1.0\n",
      "32940850    4.0\n",
      "18196050    1.0\n",
      "...         ...\n",
      "17727189    1.0\n",
      "847392      7.0\n",
      "11466371    1.0\n",
      "32905630    4.0\n",
      "19913675    1.0\n",
      "\n",
      "[31750537 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 14:12:13.056257: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-31 14:12:13.265013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753951333.342778   37622 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753951333.365282   37622 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-31 14:12:13.556821: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1753951336.860854   37622 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13499 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753951345.055792   38865 service.cc:148] XLA service 0x78f9c8004b60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753951345.056065   38865 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4080 SUPER, Compute Capability 8.9\n",
      "2025-07-31 14:12:25.078680: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1753951345.155634   38865 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-07-31 14:12:25.670859: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m    57/248052\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11:25\u001b[0m 3ms/step - accuracy: 0.3359 - loss: 19.9083   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753951346.473407   38865 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 2ms/step - accuracy: 0.8909 - loss: 0.3722\n",
      "Epoch 2/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m533s\u001b[0m 2ms/step - accuracy: 0.9231 - loss: 0.2290\n",
      "Epoch 3/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m533s\u001b[0m 2ms/step - accuracy: 0.9332 - loss: 0.1977\n",
      "Epoch 4/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1828\n",
      "Epoch 5/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 2ms/step - accuracy: 0.9400 - loss: 0.1751\n",
      "Epoch 6/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 2ms/step - accuracy: 0.9424 - loss: 0.1675\n",
      "Epoch 7/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 2ms/step - accuracy: 0.9440 - loss: 0.1626\n",
      "Epoch 8/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 2ms/step - accuracy: 0.9453 - loss: 0.1586\n",
      "Epoch 9/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 2ms/step - accuracy: 0.9465 - loss: 0.1553\n",
      "Epoch 10/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 2ms/step - accuracy: 0.9476 - loss: 0.1520\n",
      "CPU times: user 59min 34s, sys: 29min 59s, total: 1h 29min 34s\n",
      "Wall time: 1h 29min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nFor Toronto,  with 128 the accuracy was capped around 51% no matter how many epochs it ran.\\n\\nIf it's getting capped at that same range again. \\n    - You will have to check with the transormation info (Toronto Github page)\\n    - Also try to include RGB info\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Model 1 : Unc as a feature>----||\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# normalized_uncertainty = (epistemic_uncertainty - epistemic_uncertainty.min()) / (epistemic_uncertainty.max() - epistemic_uncertainty.min())\n",
    "# weights = 1 - normalized_uncertainty\n",
    "\n",
    "\"\"\"\n",
    "num_classes = 4\n",
    "classes_present = [1, 2, 5, 8]\n",
    "\"\"\"\n",
    "\n",
    "num_classes = 9\n",
    "classes_present = [0, 1, 2, 3, 4, 5, 6, 7, 8] #Toronto\n",
    "\n",
    "class_mapping = {cls: i for i, cls in enumerate(classes_present)}\n",
    "y_mapped = y_train['label'].map(class_mapping)\n",
    "y_onehot = tf.one_hot(y_mapped, depth=num_classes)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Model architecture>------------||\n",
    "model = keras.Sequential([\n",
    "    # keras.layers.Dense(128, activation='relu', input_shape=(16,)),\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(18,)), #Toronto || 18, including x,y\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(9, activation='softmax') #keras.layers.Dense(4, activation='softmax') #Toronto\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "model.fit(X_train, y_onehot, epochs=numEpochs, batch_size=128) #Toronto, changed from 128 for paris lille (batch size) || 64 was also a good option\n",
    "\n",
    "\"\"\"\n",
    "For Toronto,  with 128 the accuracy was capped around 51% no matter how many epochs it ran.\n",
    "\n",
    "If it's getting capped at that same range again. \n",
    "    - You will have to check with the transormation info (Toronto Github page)\n",
    "    - Also try to include RGB info\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 953us/step\n",
      "Test Accuracy: 95.23%\n",
      "CPU times: user 4min 32s, sys: 1min 26s, total: 5min 58s\n",
      "Wall time: 5min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Testing the model>-------------||\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_test_mapped = y_test['label'].map(class_mapping)\n",
    "y_test_mapped = y_test_mapped.to_numpy()\n",
    "accuracy = accuracy_score(y_test_mapped, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#--------------------------------------------------------------------------------------------<Load the model>----------------||\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers, models\\nfrom tensorflow import keras\\nfrom tensorflow.keras.layers import Dropout\\nfrom tensorflow.keras.models import load_model\\n\\nnumEpochs = 10\\nmodel = load_model(f\"Models/{fileName}_1_{str(numEpochs)}epoch.keras\")  # or \"my_model.h5\"\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<Save the model>----------------||\n",
    "\n",
    "# Optional: To save as a single HDF5 file instead:\n",
    "model.save(f\"Models/{fileName}_1_{str(numEpochs)}epoch.h5\")\n",
    "model.save(f\"Models/{fileName}_1_{str(numEpochs)}epoch.keras\")\n",
    "\n",
    "\"\"\"\n",
    "#--------------------------------------------------------------------------------------------<Load the model>----------------||\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "numEpochs = 10\n",
    "model = load_model(f\"Models/{fileName}_1_{str(numEpochs)}epoch.keras\")  # or \"my_model.h5\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving predicted to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m992205/992205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m918s\u001b[0m 925us/step \n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 960us/step\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<Save the output>---------------||\n",
    "\n",
    "#Train set\n",
    "yTrainProbs = model.predict(X_train)\n",
    "yTrainPred = np.argmax(yTrainProbs, axis=1)\n",
    "\n",
    "#Test set\n",
    "yTestProbs = model.predict(X_test)\n",
    "yTestPred = np.argmax(yTestProbs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31750537\n",
      "31750537\n",
      "7937635\n",
      "7937635\n",
      "Saved merged file with 39688172 points to 'features_L003_1_10epoch.csv'\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<Merge data>--------------------||\n",
    "train_df = X_train[['x', 'y', 'z']].copy()\n",
    "print(len(train_df))\n",
    "print(len(yTrainPred))\n",
    "train_df['label'] = yTrainPred\n",
    "\n",
    "test_df = X_test[['x', 'y', 'z']].copy()\n",
    "print(len(test_df))\n",
    "print(len(yTestPred))\n",
    "test_df['label'] = yTestPred\n",
    "\n",
    "combined = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "combined.to_csv(f\"Outputs/{fileName}/{fileName}_1_{str(numEpochs)}epoch.csv\", index=False)\n",
    "\n",
    "print(f\"Saved merged file with {len(combined)} points to '{fileName}_1_{str(numEpochs)}epoch.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:yellow\">2. Epistemic uncertainty + custom loss function</span>\n",
    "- #LOADPICKLESANDRUN, run this one to load the necessary files, to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pkl files from disk.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, fileName = loadPkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epistemic_uncertainty = X_train['epistemic'].values\n",
    "X_train = X_train.drop(['epistemic'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 16:27:06.552294: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-31 16:27:06.563957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753959426.575520   76539 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753959426.578828   76539 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-31 16:27:06.593218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1753959429.703936   76539 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13499 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753959447.898984   77005 service.cc:148] XLA service 0x7f908c003c50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753959447.899028   77005 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4080 SUPER, Compute Capability 8.9\n",
      "2025-07-31 16:27:28.320741: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1753959448.810196   77005 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-07-31 16:27:29.280231: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_202', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m     3/248052\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50:10\u001b[0m 27ms/step - accuracy: 0.0582 - loss: 21.5618 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753959450.242446   77005 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 2ms/step - accuracy: 0.8873 - loss: 0.0950\n",
      "Epoch 2/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 2ms/step - accuracy: 0.9189 - loss: 0.0574\n",
      "Epoch 3/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 3ms/step - accuracy: 0.9272 - loss: 0.0513\n",
      "Epoch 4/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 3ms/step - accuracy: 0.9328 - loss: 0.0470 \n",
      "Epoch 5/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 3ms/step - accuracy: 0.9370 - loss: 0.0436 \n",
      "Epoch 6/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 3ms/step - accuracy: 0.9402 - loss: 0.0412 \n",
      "Epoch 7/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 3ms/step - accuracy: 0.9421 - loss: 0.0397 \n",
      "Epoch 8/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 3ms/step - accuracy: 0.9435 - loss: 0.0387\n",
      "Epoch 9/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 3ms/step - accuracy: 0.9448 - loss: 0.0378 \n",
      "Epoch 10/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 3ms/step - accuracy: 0.9459 - loss: 0.0371 \n",
      "CPU times: user 1h 2min 45s, sys: 31min 46s, total: 1h 34min 32s\n",
      "Wall time: 1h 44min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f9236adbca0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Model 2 : Unc + Custom loss fn>||\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "normalized_uncertainty = (epistemic_uncertainty - epistemic_uncertainty.min()) / (epistemic_uncertainty.max() - epistemic_uncertainty.min())\n",
    "\n",
    "weights = 1 - normalized_uncertainty\n",
    "\n",
    "\"\"\"\n",
    "num_classes = 4\n",
    "classes_present = [1, 2, 5, 8]\n",
    "\"\"\"\n",
    "\n",
    "num_classes = 9\n",
    "classes_present = [0, 1, 2, 3, 4, 5, 6, 7, 8] #Toronto\n",
    "class_mapping = {cls: i for i, cls in enumerate(classes_present)}\n",
    "y_mapped = y_train['label'].map(class_mapping)\n",
    "y_onehot = tf.one_hot(y_mapped, depth=num_classes)\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Compute the categorical cross-entropy loss\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "        unweighted_loss = cce(y_true, y_pred)\n",
    "        \n",
    "        # Apply weights to the loss\n",
    "        weighted_loss = unweighted_loss * weights\n",
    "        return tf.reduce_mean(weighted_loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Model architecture>------------||\n",
    "model = keras.Sequential([\n",
    "    # keras.layers.Dense(128, activation='relu', input_shape=(15,)),\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(17,)), #Toronto\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    #keras.layers.Dense(4, activation='softmax') \n",
    "    keras.layers.Dense(9, activation='softmax') #Toronto\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=weighted_categorical_crossentropy(weights),  metrics=['accuracy'])\n",
    "model.fit(X_train, y_onehot, epochs=numEpochs, batch_size=128) #Toronto || batch_size=16 originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['epistemic'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 869us/step\n",
      "Test Accuracy: 92.71%\n",
      "CPU times: user 4min 22s, sys: 1min 23s, total: 5min 46s\n",
      "Wall time: 4min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Testing the model>-------------||\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_test_mapped = y_test['label'].map(class_mapping)\n",
    "y_test_mapped = y_test_mapped.to_numpy()\n",
    "accuracy = accuracy_score(y_test_mapped, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#--------------------------------------------------------------------------------------------<Load the model>----------------//\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers, models\\nfrom tensorflow import keras\\nfrom tensorflow.keras.layers import Dropout\\nfrom tensorflow.keras.models import load_model\\n\\nnumEpochs = 10\\nmodel = load_model(f\"Models/{fileName}_2_{str(numEpochs)}epoch.keras\")  # or \"my_model.h5\"\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<Save the model>----------------||\n",
    "\n",
    "# Optional: To save as a single HDF5 file instead:\n",
    "model.save(f\"Models/{fileName}_2_{str(numEpochs)}epoch.h5\")\n",
    "model.save(f\"Models/{fileName}_2_{str(numEpochs)}epoch.keras\")\n",
    "\n",
    "\"\"\"\n",
    "#--------------------------------------------------------------------------------------------<Load the model>----------------//\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "numEpochs = 10\n",
    "model = load_model(f\"Models/{fileName}_2_{str(numEpochs)}epoch.keras\")  # or \"my_model.h5\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m992205/992205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 872us/step \n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 875us/step\n",
      "31750537\n",
      "31750537\n",
      "7937635\n",
      "7937635\n",
      "Saved merged file with 39688172 points to 'features_L003_2_10epoch.csv'\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<Save the output>---------------||\n",
    "\n",
    "#Train set\n",
    "yTrainProbs = model.predict(X_train)\n",
    "yTrainPred = np.argmax(yTrainProbs, axis=1)\n",
    "\n",
    "#Test set\n",
    "yTestProbs = model.predict(X_test)\n",
    "yTestPred = np.argmax(yTestProbs, axis=1)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Merge data>--------------------||\n",
    "train_df = X_train[['x', 'y', 'z']].copy()\n",
    "print(len(train_df))\n",
    "print(len(yTrainPred))\n",
    "train_df['label'] = yTrainPred\n",
    "\n",
    "test_df = X_test[['x', 'y', 'z']].copy()\n",
    "print(len(test_df))\n",
    "print(len(yTestPred))\n",
    "test_df['label'] = yTestPred\n",
    "\n",
    "combined = pd.concat([train_df, test_df], ignore_index=True)\n",
    "combined.to_csv(f\"Outputs/{fileName}/{fileName}_2_{str(numEpochs)}epoch.csv\", index=False)\n",
    "\n",
    "print(f\"Saved merged file with {len(combined)} points to '{fileName}_2_{str(numEpochs)}epoch.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:yellow\">3. Baseline model</span>\n",
    "- #LOADPICKLESANDRUN, run this one to load the necessary files, to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pkl files from disk.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, fileName = loadPkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>eigenvalue_sum</th>\n",
       "      <th>omnivariance</th>\n",
       "      <th>eigenentropy</th>\n",
       "      <th>anisotropy</th>\n",
       "      <th>planarity</th>\n",
       "      <th>linearity</th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "      <th>surface_variation</th>\n",
       "      <th>sphericity</th>\n",
       "      <th>verticality</th>\n",
       "      <th>nx</th>\n",
       "      <th>ny</th>\n",
       "      <th>nz</th>\n",
       "      <th>epistemic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31308525</th>\n",
       "      <td>312.169006</td>\n",
       "      <td>412.657013</td>\n",
       "      <td>143.847000</td>\n",
       "      <td>0.469674</td>\n",
       "      <td>0.250471</td>\n",
       "      <td>0.574171</td>\n",
       "      <td>0.977353</td>\n",
       "      <td>0.437080</td>\n",
       "      <td>0.541052</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.619810</td>\n",
       "      <td>0.045062</td>\n",
       "      <td>0.022647</td>\n",
       "      <td>0.941772</td>\n",
       "      <td>0.014578</td>\n",
       "      <td>0.383734</td>\n",
       "      <td>0.470888</td>\n",
       "      <td>0.719572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34079256</th>\n",
       "      <td>282.249023</td>\n",
       "      <td>420.103027</td>\n",
       "      <td>139.065994</td>\n",
       "      <td>0.340488</td>\n",
       "      <td>0.212363</td>\n",
       "      <td>0.439976</td>\n",
       "      <td>0.952983</td>\n",
       "      <td>0.175902</td>\n",
       "      <td>0.778579</td>\n",
       "      <td>0.682065</td>\n",
       "      <td>0.349493</td>\n",
       "      <td>0.109340</td>\n",
       "      <td>0.047017</td>\n",
       "      <td>0.992382</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>0.342690</td>\n",
       "      <td>0.496193</td>\n",
       "      <td>0.773283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11043232</th>\n",
       "      <td>283.502991</td>\n",
       "      <td>444.824005</td>\n",
       "      <td>134.238998</td>\n",
       "      <td>0.469128</td>\n",
       "      <td>0.096462</td>\n",
       "      <td>0.574223</td>\n",
       "      <td>0.998638</td>\n",
       "      <td>0.836248</td>\n",
       "      <td>0.162569</td>\n",
       "      <td>0.311632</td>\n",
       "      <td>0.910919</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.510136</td>\n",
       "      <td>0.500974</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.980215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32940850</th>\n",
       "      <td>282.460999</td>\n",
       "      <td>419.219971</td>\n",
       "      <td>141.490005</td>\n",
       "      <td>0.472420</td>\n",
       "      <td>0.219698</td>\n",
       "      <td>0.576668</td>\n",
       "      <td>0.985258</td>\n",
       "      <td>0.509153</td>\n",
       "      <td>0.476649</td>\n",
       "      <td>0.472388</td>\n",
       "      <td>0.680714</td>\n",
       "      <td>0.028251</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.997076</td>\n",
       "      <td>0.022007</td>\n",
       "      <td>0.353302</td>\n",
       "      <td>0.501464</td>\n",
       "      <td>0.705953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18196050</th>\n",
       "      <td>292.648987</td>\n",
       "      <td>435.911011</td>\n",
       "      <td>134.365997</td>\n",
       "      <td>0.479123</td>\n",
       "      <td>0.077661</td>\n",
       "      <td>0.583332</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>0.857077</td>\n",
       "      <td>0.142412</td>\n",
       "      <td>0.303022</td>\n",
       "      <td>0.923065</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.492432</td>\n",
       "      <td>0.493903</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.980222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17727189</th>\n",
       "      <td>287.860962</td>\n",
       "      <td>442.264008</td>\n",
       "      <td>134.354004</td>\n",
       "      <td>0.505623</td>\n",
       "      <td>0.120630</td>\n",
       "      <td>0.608914</td>\n",
       "      <td>0.997708</td>\n",
       "      <td>0.991098</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.995538</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.508979</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.980222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847392</th>\n",
       "      <td>259.846008</td>\n",
       "      <td>542.855011</td>\n",
       "      <td>136.074997</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.623508</td>\n",
       "      <td>0.752522</td>\n",
       "      <td>0.787206</td>\n",
       "      <td>0.243372</td>\n",
       "      <td>0.550521</td>\n",
       "      <td>0.402431</td>\n",
       "      <td>0.542980</td>\n",
       "      <td>0.378741</td>\n",
       "      <td>0.212794</td>\n",
       "      <td>0.669778</td>\n",
       "      <td>0.318830</td>\n",
       "      <td>0.935793</td>\n",
       "      <td>0.334890</td>\n",
       "      <td>0.951129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11466371</th>\n",
       "      <td>287.469971</td>\n",
       "      <td>436.969971</td>\n",
       "      <td>134.311005</td>\n",
       "      <td>0.504488</td>\n",
       "      <td>0.111190</td>\n",
       "      <td>0.607642</td>\n",
       "      <td>0.998216</td>\n",
       "      <td>0.967420</td>\n",
       "      <td>0.031009</td>\n",
       "      <td>0.256657</td>\n",
       "      <td>0.983464</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.511646</td>\n",
       "      <td>0.501143</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.980222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32905630</th>\n",
       "      <td>281.354980</td>\n",
       "      <td>423.684021</td>\n",
       "      <td>134.513000</td>\n",
       "      <td>0.366577</td>\n",
       "      <td>0.053321</td>\n",
       "      <td>0.451639</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.220751</td>\n",
       "      <td>0.778767</td>\n",
       "      <td>0.726146</td>\n",
       "      <td>0.362114</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.486507</td>\n",
       "      <td>0.489251</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.488311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19913675</th>\n",
       "      <td>297.515015</td>\n",
       "      <td>435.852997</td>\n",
       "      <td>134.335007</td>\n",
       "      <td>0.465869</td>\n",
       "      <td>0.081159</td>\n",
       "      <td>0.569347</td>\n",
       "      <td>0.999216</td>\n",
       "      <td>0.699765</td>\n",
       "      <td>0.299589</td>\n",
       "      <td>0.378023</td>\n",
       "      <td>0.823467</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.501816</td>\n",
       "      <td>0.497901</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.980220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31750537 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   x           y           z  eigenvalue_sum  omnivariance  \\\n",
       "31308525  312.169006  412.657013  143.847000        0.469674      0.250471   \n",
       "34079256  282.249023  420.103027  139.065994        0.340488      0.212363   \n",
       "11043232  283.502991  444.824005  134.238998        0.469128      0.096462   \n",
       "32940850  282.460999  419.219971  141.490005        0.472420      0.219698   \n",
       "18196050  292.648987  435.911011  134.365997        0.479123      0.077661   \n",
       "...              ...         ...         ...             ...           ...   \n",
       "17727189  287.860962  442.264008  134.354004        0.505623      0.120630   \n",
       "847392    259.846008  542.855011  136.074997        0.623794      0.623508   \n",
       "11466371  287.469971  436.969971  134.311005        0.504488      0.111190   \n",
       "32905630  281.354980  423.684021  134.513000        0.366577      0.053321   \n",
       "19913675  297.515015  435.852997  134.335007        0.465869      0.081159   \n",
       "\n",
       "          eigenentropy  anisotropy  planarity  linearity      PCA1      PCA2  \\\n",
       "31308525      0.574171    0.977353   0.437080   0.541052  0.510000  0.619810   \n",
       "34079256      0.439976    0.952983   0.175902   0.778579  0.682065  0.349493   \n",
       "11043232      0.574223    0.998638   0.836248   0.162569  0.311632  0.910919   \n",
       "32940850      0.576668    0.985258   0.509153   0.476649  0.472388  0.680714   \n",
       "18196050      0.583332    0.999328   0.857077   0.142412  0.303022  0.923065   \n",
       "...                ...         ...        ...        ...       ...       ...   \n",
       "17727189      0.608914    0.997708   0.991098   0.006844  0.247191  0.995538   \n",
       "847392        0.752522    0.787206   0.243372   0.550521  0.402431  0.542980   \n",
       "11466371      0.607642    0.998216   0.967420   0.031009  0.256657  0.983464   \n",
       "32905630      0.451639    0.999465   0.220751   0.778767  0.726146  0.362114   \n",
       "19913675      0.569347    0.999216   0.699765   0.299589  0.378023  0.823467   \n",
       "\n",
       "          surface_variation  sphericity  verticality        nx        ny  \\\n",
       "31308525           0.045062    0.022647     0.941772  0.014578  0.383734   \n",
       "34079256           0.109340    0.047017     0.992382  0.025407  0.342690   \n",
       "11043232           0.002182    0.001362     0.000207  0.510136  0.500974   \n",
       "32940850           0.028251    0.014742     0.997076  0.022007  0.353302   \n",
       "18196050           0.001066    0.000672     0.000189  0.492432  0.493903   \n",
       "...                     ...         ...          ...       ...       ...   \n",
       "17727189           0.003385    0.002292     0.000162  0.508979  0.499412   \n",
       "847392             0.378741    0.212794     0.669778  0.318830  0.935793   \n",
       "11466371           0.002668    0.001784     0.000274  0.511646  0.501143   \n",
       "32905630           0.001291    0.000535     0.000595  0.486507  0.489251   \n",
       "19913675           0.001358    0.000784     0.000015  0.501816  0.497901   \n",
       "\n",
       "                nz  epistemic  \n",
       "31308525  0.470888   0.719572  \n",
       "34079256  0.496193   0.773283  \n",
       "11043232  0.000104   0.980215  \n",
       "32940850  0.501464   0.705953  \n",
       "18196050  0.000094   0.980222  \n",
       "...            ...        ...  \n",
       "17727189  0.000081   0.980222  \n",
       "847392    0.334890   0.951129  \n",
       "11466371  0.000137   0.980222  \n",
       "32905630  0.000298   0.488311  \n",
       "19913675  0.000008   0.980220  \n",
       "\n",
       "[31750537 rows x 18 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this one if needed\n",
    "X_train = X_train.drop(['epistemic'], axis=1)\n",
    "X_test = X_test.drop(['epistemic'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 2ms/step - accuracy: 0.8896 - loss: 0.3724\n",
      "Epoch 2/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2ms/step - accuracy: 0.9206 - loss: 0.2360\n",
      "Epoch 3/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 2ms/step - accuracy: 0.9286 - loss: 0.2123\n",
      "Epoch 4/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 2ms/step - accuracy: 0.9346 - loss: 0.1938\n",
      "Epoch 5/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 2ms/step - accuracy: 0.9383 - loss: 0.1821\n",
      "Epoch 6/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 2ms/step - accuracy: 0.9407 - loss: 0.1741\n",
      "Epoch 7/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 2ms/step - accuracy: 0.9421 - loss: 0.1710\n",
      "Epoch 8/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 2ms/step - accuracy: 0.9436 - loss: 0.1666\n",
      "Epoch 9/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 2ms/step - accuracy: 0.9441 - loss: 0.1653\n",
      "Epoch 10/10\n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 2ms/step - accuracy: 0.9450 - loss: 0.1627\n",
      "CPU times: user 1h 47s, sys: 28min 52s, total: 1h 29min 39s\n",
      "Wall time: 1h 22min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f92d39031f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Model 3 : Baseline>------------||\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# normalized_uncertainty = (epistemic_uncertainty - epistemic_uncertainty.min()) / (epistemic_uncertainty.max() - epistemic_uncertainty.min())\n",
    "\n",
    "# weights = 1 - normalized_uncertainty\n",
    "\n",
    "\"\"\"\n",
    "num_classes = 4\n",
    "classes_present = [1, 2, 5, 8]\n",
    "\"\"\"\n",
    "\n",
    "num_classes = 9\n",
    "classes_present = [0, 1, 2, 3, 4, 5, 6, 7, 8] #Toronto\n",
    "class_mapping = {cls: i for i, cls in enumerate(classes_present)}\n",
    "y_mapped = y_train['label'].map(class_mapping)\n",
    "y_onehot = tf.one_hot(y_mapped, depth=num_classes)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Model architecture>------------||\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # keras.layers.Dense(128, activation='relu', input_shape=(15,)),\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(17,)), #Toronto\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    #keras.layers.Dense(4, activation='softmax') \n",
    "    keras.layers.Dense(9, activation='softmax') #Toronto\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "model.fit(X_train, y_onehot, epochs=numEpochs, batch_size=128) #Toronto || batch_size=16 originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 883us/step\n",
      "Test Accuracy: 94.67%\n",
      "CPU times: user 3min 37s, sys: 1min 20s, total: 4min 57s\n",
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Testing the model>-------------||\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "y_test_mapped = y_test['label'].map(class_mapping)\n",
    "y_test_mapped = y_test_mapped.to_numpy()\n",
    "accuracy = accuracy_score(y_test_mapped, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#--------------------------------------------------------------------------------------------<Load the model>----------------//\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers, models\\nfrom tensorflow import keras\\nfrom tensorflow.keras.layers import Dropout\\nfrom tensorflow.keras.models import load_model\\n\\nnumEpochs = 10\\nmodel = load_model(f\"Models/{fileName}_3_{str(numEpochs)}epoch.keras\")  # or \"my_model.h5\"\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<Save the model>----------------||\n",
    "\n",
    "# Optional: To save as a single HDF5 file instead:\n",
    "model.save(f\"Models/{fileName}_3_{str(numEpochs)}epoch.h5\")\n",
    "model.save(f\"Models/{fileName}_3_{str(numEpochs)}epoch.keras\")\n",
    "\n",
    "\"\"\"\n",
    "#--------------------------------------------------------------------------------------------<Load the model>----------------//\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "numEpochs = 10\n",
    "model = load_model(f\"Models/{fileName}_3_{str(numEpochs)}epoch.keras\")  # or \"my_model.h5\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m992205/992205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m900s\u001b[0m 907us/step \n",
      "\u001b[1m248052/248052\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 903us/step\n",
      "31750537\n",
      "31750537\n",
      "7937635\n",
      "7937635\n",
      "Saved merged file with 39688172 points to 'features_L003_3_10epoch.csv'\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------<Save the output>---------------||\n",
    "\n",
    "#Train set\n",
    "yTrainProbs = model.predict(X_train)\n",
    "yTrainPred = np.argmax(yTrainProbs, axis=1)\n",
    "\n",
    "#Test set\n",
    "yTestProbs = model.predict(X_test)\n",
    "yTestPred = np.argmax(yTestProbs, axis=1)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------<Merge data>--------------------||\n",
    "train_df = X_train[['x', 'y', 'z']].copy()\n",
    "print(len(train_df))\n",
    "print(len(yTrainPred))\n",
    "train_df['label'] = yTrainPred\n",
    "\n",
    "test_df = X_test[['x', 'y', 'z']].copy()\n",
    "print(len(test_df))\n",
    "print(len(yTestPred))\n",
    "test_df['label'] = yTestPred\n",
    "\n",
    "combined = pd.concat([train_df, test_df], ignore_index=True)\n",
    "combined.to_csv(f\"Outputs/{fileName}/{fileName}_3_{str(numEpochs)}epoch.csv\", index=False)\n",
    "\n",
    "print(f\"Saved merged file with {len(combined)} points to '{fileName}_3_{str(numEpochs)}epoch.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAejdJREFUeJzt3Xd4FOXexvF7N5UAodcQiOKRolIOCga7UhQbCIoEpYh4joKi0aMgEpqKvipgQTh6KCpSFBEbghiKBQQVUVREUQQpCSCSQALJsjvvH+MmWdI2ZbI7yfdzXXPt7uzM7G/zsCT3Ps884zAMwxAAAAAAACh3zkAXAAAAAABAZUXoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAKAYDodDEyZMKPF+v//+uxwOh+bNm1fuNZXUhAkT5HA4SrXvvHnz5HA49Pvvv5dvUTZWlp8nAKBqIXQDAGzBG/wcDoc+++yzfM8bhqHY2Fg5HA5dc801AaiwdOLi4nLeV1FLMAT3QPCGW6fTqT/++CPf8+np6apWrZocDodGjhxZqtd4/PHHtWzZsjJWCgBAwQjdAABbiYyM1IIFC/KtX7dunfbs2aOIiIgAVFV606dP12uvvZazDBgwQJI0bdo0n/UXX3xxmV7nkUce0fHjx0u176233qrjx4+rRYsWZaqhLCIiIrRw4cJ865cuXVrmY5cmdJfl5wkAqFpCA10AAAAl0atXL7355pt67rnnFBqa+2tswYIF6tSpkw4dOhTA6kqud+/ePo9TUlK0cOFC9e7dW3FxcYXul5GRoerVq/v9OqGhoT4/r5IICQlRSEhIqfYtL7169dLChQv14IMP+qxfsGCBrr76ar311lsVUof3516WnycAoGqhpxsAYCsDBgzQn3/+qVWrVuWsy87O1pIlS5SQkFDgPhkZGbr//vsVGxuriIgItWrVSk8//bQMw/DZLisrS/fdd58aNGigmjVr6rrrrtOePXsKPObevXt12223qVGjRoqIiNBZZ52lOXPmlN8bzWPIkCGqUaOGfv31V/Xq1Us1a9bUwIEDJUmffvqpbrzxRjVv3lwRERGKjY3Vfffdl68XtqBzkL1DspctW6azzz47532sWLHCZ7uCzumOi4vTNddco88++0ydO3dWZGSkTj/9dL366qv56v/uu+90ySWXqFq1amrWrJkeffRRzZ07t0TniSckJGjLli366aefctalpKRo9erVhbZ7VlaWxo8frzPOOCPnZ/Pggw8qKyvL52eQkZGhV155JWco/5AhQ3x+Zj/++KMSEhJUp04dXXjhhYX+PCVp/vz56ty5s6KiolSnTh1dfPHF+uijj3Ke/+qrr9SzZ0/Vr19f1apV02mnnabbbrvNr58BAMCe+IoWAGArcXFxio+P18KFC3XVVVdJkj788EOlpaXp5ptv1nPPPeezvWEYuu6667RmzRoNGzZMHTp00MqVK/Wf//xHe/fu1bRp03K2vf322zV//nwlJCSoa9euWr16ta6++up8NaSmpur888/PCa0NGjTQhx9+qGHDhik9PV333ntvub/vkydPqmfPnrrwwgv19NNPKyoqSpL05ptvKjMzU3feeafq1aunTZs26fnnn9eePXv05ptvFnvczz77TEuXLtVdd92lmjVr6rnnnlPfvn21e/du1atXr8h9d+zYoX79+mnYsGEaPHiw5syZoyFDhqhTp04666yzJJlfTlx22WVyOBwaM2aMqlevrv/9738lPg3g4osvVrNmzbRgwQJNmjRJkrR48WLVqFGjwDbyeDy67rrr9Nlnn+mOO+5QmzZttHXrVk2bNk0///xzznDy1157Tbfffrs6d+6sO+64Q5LUsmVLn2PdeOON+sc//qHHH3883xc1eU2cOFETJkxQ165dNWnSJIWHh2vjxo1avXq1evTooQMHDqhHjx5q0KCBRo8erdq1a+v3338vlyHyAIAgZgAAYANz5841JBlffvml8cILLxg1a9Y0MjMzDcMwjBtvvNG47LLLDMMwjBYtWhhXX311zn7Lli0zJBmPPvqoz/H69etnOBwOY8eOHYZhGMaWLVsMScZdd93ls11CQoIhyRg/fnzOumHDhhlNmjQxDh065LPtzTffbNSqVSunrp07dxqSjLlz5/r9Pp966ilDkrFz586cdYMHDzYkGaNHj863vfe18poyZYrhcDiMXbt25awbP368ceqvfUlGeHh4zs/AMAzj22+/NSQZzz//fM46788+b00tWrQwJBmffPJJzroDBw4YERERxv3335+z7u677zYcDofxzTff5Kz7888/jbp16+Y7ZkG8dR88eNB44IEHjDPOOCPnufPOO88YOnRoznsZMWJEznOvvfaa4XQ6jU8//dTneLNmzTIkGZ9//nnOuurVqxuDBw8u9LUHDBhQ6HNev/zyi+F0Oo0+ffoYbrfbZ1uPx2MYhmG8/fbbOf+GAQBVB8PLAQC2c9NNN+n48eN6//33dfToUb3//vuFDjFevny5QkJCdM899/isv//++2UYhj788MOc7STl2+7UXmvDMPTWW2/p2muvlWEYOnToUM7Ss2dPpaWlafPmzeX0Tn3deeed+dZVq1Yt535GRoYOHTqkrl27yjAMffPNN8Ues1u3bj49u+3atVN0dLR+++23Yvdt27atLrroopzHDRo0UKtWrXz2XbFiheLj49WhQ4ecdXXr1s0ZHl8SCQkJ2rFjh7788suc28La/c0331SbNm3UunVrnza6/PLLJUlr1qzx+3X//e9/F7vNsmXL5PF4lJSUJKfT988r7zD02rVrS5Lef/99uVwuv18fAGBvDC8HANhOgwYN1K1bNy1YsECZmZlyu93q169fgdvu2rVLTZs2Vc2aNX3Wt2nTJud5763T6cw3tLhVq1Y+jw8ePKgjR47opZde0ksvvVTgax44cKBU76sooaGhatasWb71u3fvVlJSkt5991399ddfPs+lpaUVe9zmzZvnW1enTp18xyrtvrt27VJ8fHy+7c4444xij3+qjh07qnXr1lqwYIFq166txo0b54ToU/3yyy/atm2bGjRoUODzJWmj0047rdhtfv31VzmdTrVt27bQbS655BL17dtXEydO1LRp03TppZeqd+/eSkhIsN2s+wAA/xG6AQC2lJCQoOHDhyslJUVXXXVVTi+i1TwejyTplltu0eDBgwvcpl27duX+uhEREfl6UN1ut7p3767Dhw/roYceUuvWrVW9enXt3btXQ4YMyam1KIXNSm4Uce5yeexbWgkJCZo5c6Zq1qyp/v375/uZeHk8Hp1zzjmaOnVqgc/Hxsb6/Zp5RxOUhcPh0JIlS/TFF1/ovffe08qVK3XbbbfpmWee0RdffKEaNWqUy+sAAIILoRsAYEt9+vTRv/71L33xxRdavHhxodu1aNFCH3/8sY4ePerT2+2dBdt77ekWLVrI4/Ho119/9end3r59u8/xvDObu91udevWrTzfUolt3bpVP//8s1555RUNGjQoZ33emd0DrUWLFtqxY0e+9QWt80dCQoKSkpK0f/9+vfbaa4Vu17JlS3377be64oorCpxlPK/invdHy5Yt5fF49OOPP/oMpS/I+eefr/PPP1+PPfaYFixYoIEDB2rRokW6/fbby1wHACD4cE43AMCWatSooZkzZ2rChAm69tprC92uV69ecrvdeuGFF3zWT5s2TQ6HI2cGdO/tqbOfT58+3edxSEiI+vbtq7feekvff/99vtc7ePBgad5OqXh7mvP2LBuGoWeffbbCaihOz549tWHDBm3ZsiVn3eHDh/X666+X6ngtW7bU9OnTNWXKFHXu3LnQ7W666Sbt3btXL7/8cr7njh8/royMjJzH1atX15EjR0pVj1fv3r3ldDo1adKkfCMMvO3z119/5RsF4A3oeS9jBgCoXOjpBgDYVmHDu/O69tprddlll2ns2LH6/fff1b59e3300Ud65513dO+99+acw92hQwcNGDBAL774otLS0tS1a1clJycX2CP7xBNPaM2aNerSpYuGDx+utm3b6vDhw9q8ebM+/vhjHT58uNzfa0Fat26tli1b6oEHHtDevXsVHR2tt956y6/zsSvKgw8+qPnz56t79+66++67cy4Z1rx5cx0+fLhUvcyjRo0qdptbb71Vb7zxhv79739rzZo1uuCCC+R2u/XTTz/pjTfe0MqVK3XuuedKkjp16qSPP/5YU6dOVdOmTXXaaaepS5cuJarpjDPO0NixYzV58mRddNFFuuGGGxQREaEvv/xSTZs21ZQpU/TKK6/oxRdfVJ8+fdSyZUsdPXpUL7/8sqKjo9WrV68S/xwAAPZA6AYAVGpOp1PvvvuukpKStHjxYs2dO1dxcXF66qmndP/99/tsO2fOHDVo0ECvv/66li1bpssvv1wffPBBvvN/GzVqpE2bNmnSpElaunSpXnzxRdWrV09nnXWWnnzyyQp7b2FhYXrvvfd0zz33aMqUKYqMjFSfPn00cuRItW/fvsLqKEpsbKzWrFmje+65R48//rgaNGigESNGqHr16rrnnnsUGRlpyes6nU4tW7ZM06ZN06uvvqq3335bUVFROv300zVq1CideeaZOdtOnTpVd9xxhx555BEdP35cgwcPLnHolqRJkybptNNO0/PPP6+xY8cqKipK7dq106233irJnEht06ZNWrRokVJTU1WrVi117txZr7/+ul+TtQEA7MlhWDnbCQAAQAHuvfde/fe//9WxY8cKnZANAIDKgHO6AQCApY4fP+7z+M8//9Rrr72mCy+8kMANAKj0GF4OAAAsFR8fr0svvVRt2rRRamqqZs+erfT0dI0bNy7QpQEAYDlCNwAAsFSvXr20ZMkSvfTSS3I4HPrnP/+p2bNn6+KLLw50aQAAWI5zugEAAAAAsAjndAMAAAAAYBFCNwAAAAAAFqly53R7PB7t27dPNWvWlMPhCHQ5AAAAAAAbMgxDR48eVdOmTeV0Ft6fXeVC9759+xQbGxvoMgAAAAAAlcAff/yhZs2aFfp8lQvdNWvWlGT+YKKjowNcDYrjcrn00UcfqUePHgoLCwt0OfAT7WZPtJs90W72RLvZD21mT7SbPdml3dLT0xUbG5uTMQtT5UK3d0h5dHQ0odsGXC6XoqKiFB0dHdQfOPii3eyJdrMn2s2eaDf7oc3siXazJ7u1W3GnLTORGgAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAIKC2y2tW+fQJ5/EaN06h9zuQFdUdqGBLgAAAABA1eV2S59+Ku3fLzVpIl10kRQSEuiqCke91lm6VBo1StqzJ1TSuZo6VWrWTHr2WemGGwJdXenR0w0AAAAUwW49b263tHattHCheRvM9S5dKsXFSZddJiUkmLdxceb6YES91lm6VOrXT9qzx3f93r3m+mCs2V+EbgAAAFQoO4bC7t1DNXXquerePTRoQ4tEyLIS9VrH7TZ7uA0j/3PedffeG9z/VxSF4eUAAKBC2GmIo0S9VskdPpq7LliHj3pDy6lBwBtaliwJrprtVG9xIcvhMEPW9deX7N+x93iGYS4nT0put0MuV+66U7fx57HbLd19d9H13n231LGjed/j8V0MI/86f54r7b4nT0qJiUWH2GHDpN9+y22PvPuf+rigdeW5zZ9/5v9y4NSa//jD/D/u0kv9//cQLAjdAAD8zS6hxSvvkNfq1R267LLgrddOQUuiXqsQCkvn1GBVUJDJzpZGjiy63rvukho2NB+7XL7LyZP515V0fUm2PXpU2rev6Pf8xx9StWqS0+lfQC5YmKTryvDT949hmO/n9NMtf6lyc+SI9J//BLqKktm/P9AVlA6hGwBgGTuFWLuEFi87TTZjp6AlUa9V/AmxI0ZILVvm9lCavZS59099XNRzZd123z7/et7OPluqUcPansDCA6X/DENKTTX/H7YTlyvQFZRMWJi5OJ25i8Ph+9jf58qyb0qK9N13xdd7wQXmZ87pNH8/5z3GqY+t3Oann6TJk4uvt0mTsrdRIBC6AcBGCLHWsEto8bJTvf4ErXvukeLjc7f3ho5ALC6X9PTTxQ/J3L1bCg+XQkMLX8LCCn/ODGw19MsvUlRU0ccJCTF/TqX9+d57r3Tddea2WVnSiRPmbd77hd2W5zYZGeZtYQzDDAodOhT1Lyr4/PRToCsomYYNpdq1c8Nh3sX777a4dSVdX9C6rVvNnvniLFokde1q3nc4chd/H5886dLHH69S9+7dFR4eVuL9vcu6dVL37sXX+9FHwTH8ee1a83z+4jz6aHDU63ZLc+eav8cK+v/M4TD/hrDbl0ZehG4AVRoh1hqVLRR6Q4v3vD5vj1h53vd3W5dLeuyxokPh4MHS6tW+9QYyxBbVU2UY5r+Lpk3LpTkrxJEj0n33lfUoYZKu8HvrkJCCg/zJk9KhQ4Xv5+2NDQsra70VJzra7Dn2vmfvFw+nfhFR3P2yPvfbb9JzzxVf72OPSe3b+9/Ll/dxSbYt7vEnn0iXX158vYsXB0fIuuAC6Yknig9Z/fqV7feyyyXVqOFSnTpl+xxcdplZj11C4UUX2avekBDz75l+/cza8tbs/RJk+vTg/RutOIRuAFUWIdYa/vZsdu6cG8qys/PfFrTOiueOHPFvCKmdQsuxY9KMGYGuouTyBolALb//Lq1ZU3yt8fFSTIzv8OS8i/c81sIXQ5mZ2XI4wnXypCNnvcdT8Ot5v8Qoqqe4JMLDpchIKSIi9zbv/cJuy/Lcli3mbNrFeeed4AiFbrf5f29xoeWhh4IjCFx8MSHLStRrvRtuMP+eKehvs+nTg+fvnNJwGEZ5nCViH+np6apVq5bS0tIUHR0d6HJQDJfLpeXLl6tXr14Ks9NfvFWY2y2tWXNSH364RVdd1UGXXRYaVP+hexUWYr2/iIItxMbFFR4MHQ6zl/Crr8xtveEy75I3dBa2HD/u1tat23Xaaa3kdocUu31hx09LM88brEq8QdHbQ2bFfe/t7t3S558XX1Pv3lK7doENsCEh0qZN0s03F1/v6tX+DYW0mr9DMtesKVswLOz3m3dovb9B/osvpH/9q/jXW7rU7AWNiDADtzMAF431/l9WXCjcuTN4goD3d4VUcGgJpt8Vkv3qlQr+Ajw2tvxCVnn/LWl1veXNbvVK9vlbUvI/WxK6EdQI3fZil55jf0JsUX/4nXpeZEnObyzN+ZF//mn2vlU2DocZAMLCzBAQHp57/9RbK5/74Qf/hgovW2b2JJ0aios639YKFRUKy4vdglZF1Vtev9/s9vMlFFrPbvVK1p7qZcXfknY6NU2yX72SfTKAv9mS4eUAykUghz8bhtnDmpFhLpmZufcLWvf99/4NJz79dPOX0qmB+ORJa95HeQgNzQ2UJV1CQz1KTd2tli1jFRkZ4vNc3qBa3LJ1q3lZmuKsXh0cofDyy6Vnnik+tFxzTXD8kcJ5etaiXmvZcfjoDTeYlwWzS8+bt147hayQkOD4feAv6kVJEbqBIGeHbyf9OYd31CjzP/wTJ/wLxv5u413ndpf/+9q9u/htnE7rz4fctk164IHia0lO9m8SncK4XG4tX/6tevWKUVhY6f+RxcdLjz9OKLSK3eqV7Be0qNdadg2Fl1xiKCNjry65pH1Q1yoRsoBgQ+gGgligh2u73VJ6urmkpRV+++OPxfcc79kj1atnfc1hYVL16uYSFZV7P+/jtDTp/feLP9a0adL55xcdiEMr4H/Rnj3NP5yLC7GXXGJ9Lf4gFFrPbvVK9gta1GstQiGAqoTQDQSpsgzXNgyz97eooFzUc97bjIzyf19OZ+Gh2N91RW3jz2k//p4DeffdwfEHKyG2YtgttNhtyKtkv6BFvQCA8kDoBoKQ221eUqmo6/AOGiS98YZ5aaBTA3N6evkOt46IMK+bWqtW/ttataS//pLmzy/+OCtXSt27V+ykUwUhxFYMu4VYyX6hxW5DXgEAqIoI3ahyAnWOtMcjHT4sHThgLqmphd/ft88897koGRnS4sVFb+N0Fh6WS3IbEVH067jd5ozKxfUcX3FF4AO3FyG2YtgtxAIAAJQ3QjeqlPI+R/rECf9C9IED0sGD5T/Z18CB5qWDCgvM1atXTMi1Y8+xRIgFAACA9QjdqDL8OUe6Tx9zqLQ/IfrAAXMYd0nVqSM1bJi7NGqU//7Onebw8eLcfnvwBDA79hxLhFgAAABYi9CNKiEtTRoxouhzpG+80eyVLWlvdFhY/vBcWJhu0MC8jnFx4uOlhx+2zyWXvOw4sRMAAABgpYCH7hkzZuipp55SSkqK2rdvr+eff16dO3cucFuXy6UpU6bolVde0d69e9WqVSs9+eSTuvLKKyu4auQVyOtIHz9uvu6+fUUvR48WfyyPJ/d+7drFB2jv/Vq1yn8It12Ha0tM7AQAAADkFdDQvXjxYiUmJmrWrFnq0qWLpk+frp49e2r79u1q2LBhvu0feeQRzZ8/Xy+//LJat26tlStXqk+fPlq/fr06duwYgHcAq64j7XJJKSnS7t0OffFFE+3a5VRKSv4w/ddfZX8PeT3/vDR8ePETh1UEuw7XBgAAAJAroKF76tSpGj58uIYOHSpJmjVrlj744APNmTNHo0ePzrf9a6+9prFjx6pXr16SpDvvvFMff/yxnnnmGc3353pFKFeluY60222eC11cz/SBA949QiUVPPLBKzJSiomRmjYtfNmxQ7r66uLf09lnB0fg9rLjRF8AAAAAcgUsdGdnZ+vrr7/WmDFjctY5nU5169ZNGzZsKHCfrKwsRUZG+qyrVq2aPvvss0JfJysrS1lZWTmP0/+e+crlcsnlcpXlLVRp5nWkQ/8O3L5jq811hoYOlT780KOUFIf275f273coJUXyePwbix0WZqhxY0NRUUfUunW0YmIcatJEatLEUNOmube1axc/vLt5cykmJlT79kmGkX9jh8NQTIx0/vknFYz/LC64IPe+x+M7FD4YeT9bfMbshXazJ9rNnmg3+6HN7Il2sye7tJu/9TkMo6Bpmqy3b98+xcTEaP369YqPj89Z/+CDD2rdunXauHFjvn0SEhL07bffatmyZWrZsqWSk5N1/fXXy+12+wTrvCZMmKCJEyfmW79gwQJFRUWV3xuqYrZuradx4y4s1b5Op6HatU+obt3cpU4d38d1655QzZrZcjrLr+YNG5roySfP+/tR3uBtfgQeeuhLxcfvL78XBAAAAFBpZWZmKiEhQWlpaYqOji50u4BPpFYSzz77rIYPH67WrVvL4XCoZcuWGjp0qObMmVPoPmPGjFFiYmLO4/T0dMXGxqpHjx5F/mBQtPR0/3qrr7/eo+7dDZ/e6YYNpdDQUEk1/l4K53K5tGrVKnXv3l1hYWFlqrlXL+mf/3QrMTFEe/fmrm/WTHrmGbf69OkoibkBykN5thsqDu1mT7SbPdFu9kOb2RPtZk92abd0P68fHLDQXb9+fYWEhCg1NdVnfWpqqho3blzgPg0aNNCyZct04sQJ/fnnn2ratKlGjx6t008/vdDXiYiIUEQBJ+mGhYUFdQMGs6NHpeXL/dv23nud5XIN5PJqr5tukvr2PfUcaYdCQmz1/ZNt8DmzJ9rNnmg3e6Ld7Ic2syfazZ6Cvd38ra0cB++WTHh4uDp16qTk5OScdR6PR8nJyT7DzQsSGRmpmJgYnTx5Um+99Zauv/56q8uFzHO1FyyQWreWFi0qeluHQ4qNDb7rSEvmJGSXXioNGGDeMikZAAAAAKsELHRLUmJiol5++WW98sor2rZtm+68805lZGTkzGY+aNAgn4nWNm7cqKVLl+q3337Tp59+qiuvvFIej0cPPvhgoN5ClfHdd9Ill0gDB5qzi7dsKY0ZY4brUycxC/brSAMAAABARQnomNr+/fvr4MGDSkpKUkpKijp06KAVK1aoUaNGkqTdu3fLmWcmrRMnTuiRRx7Rb7/9pho1aqhXr1567bXXVLt27QC9g8rvr7+kpCTpxRfNGbOrVZPGjpXuv9+8VNe553IdaQAAAAAoTMBPZB05cqRGjhxZ4HNr1671eXzJJZfoxx9/rICq4HZLc+ZIDz8sHTpkrrvxRunpp83Lb3lxHWkAAAAAKFzAQzeCz8aN0siR0ldfmY/btpWef166/PKCt/eeIw0AAAAA8BXQc7oRXFJTpaFDpfPPNwN3dLQ0bZq0ZUvhgRsAAAAAUDh6uiGXS5oxQxo/XvJeam7IEOmJJ6S/T68HAAAAAJQCobuKW7NGuvtu6YcfzMedOkkvvGD2dgMAAAAAyobh5VXUH39I/fubw8Z/+EGqV0/673/N87kJ3AAAAABQPujprmJOnJCeeUZ6/HEpM1NyOqU775QmTZLq1g10dQAAAABQuRC6q5D335fuvVf69Vfz8YUXmkPJ27cPaFkAAAAAUGkxvLwK+OUX6eqrpWuvNQN3kybS669Ln3xC4AYAAAAAKxG6K7GMDGnsWOnss6Xly6WwMOnBB6Xt26WEBMnhCHSFAAAAAFC5Mby8EjIM6Y03pAcekPbsMdf16CE995zUqlVgawMAAACAqoTQXcl8/715CbC1a83HcXHStGnS9dfTsw0AAAAAFY3h5ZXEkSPmJGkdOpiBOzJSmjhR+vFHqXdvAjcAAAAABAI93Tbn8Ujz5kmjR0sHD5rrbrjBvCxYXFwgKwMAAAAAELpt7MsvpZEjpU2bzMetW5vnbXfvHti6AAAAAAAmhpfb0MGD0vDhUpcuZuCuUUN66inp228J3AAAAAAQTOjpDkJut/Tpp9L+/eY1tS+6SAoJkU6elGbNksaNM8/hlqRbb5WefNLcDgAAAAAQXAjdQWbpUmnUqNxLfUlSs2bSv/5lXgZs61ZzXYcO0gsvSBdcEJAyAQAAAAB+IHQHkaVLpX79zOts57Vnj9m7LUl160qPPWYOLw8JqfgaAQAAAAD+I3QHCbfb7OE+NXDnVb26tG2b1LBhxdUFAAAAACg9JlILEp9+6jukvCAZGeZ1twEAAAAA9kDoDhL795fvdgAAAACAwCN0Bwl/Zx9nlnIAAAAAsA9Cd5C46CJzlnKHo+DnHQ4pNtbcDgAAAABgD4TuIBESIj37rHn/1ODtfTx9OjOWAwAAAICdELqDyA03SEuWSDExvuubNTPX33BDYOoCAAAAAJQOlwwLMjfcIF1/vTmb+f795jncF11EDzcAAAAA2BGhOwiFhEiXXhroKgAAAAAAZcXwcgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIgEP3TNmzFBcXJwiIyPVpUsXbdq0qcjtp0+frlatWqlatWqKjY3VfffdpxMnTlRQtQAAAAAA+C+goXvx4sVKTEzU+PHjtXnzZrVv3149e/bUgQMHCtx+wYIFGj16tMaPH69t27Zp9uzZWrx4sR5++OEKrhwAAAAAgOIFNHRPnTpVw4cP19ChQ9W2bVvNmjVLUVFRmjNnToHbr1+/XhdccIESEhIUFxenHj16aMCAAcX2jgMAAAAAEAihgXrh7Oxsff311xozZkzOOqfTqW7dumnDhg0F7tO1a1fNnz9fmzZtUufOnfXbb79p+fLluvXWWwt9naysLGVlZeU8Tk9PlyS5XC65XK5yejewireNaCt7od3siXazJ9rNnmg3+6HN7Il2sye7tJu/9TkMwzAsrqVA+/btU0xMjNavX6/4+Pic9Q8++KDWrVunjRs3Frjfc889pwceeECGYejkyZP697//rZkzZxb6OhMmTNDEiRPzrV+wYIGioqLK/kYAAAAAAFVOZmamEhISlJaWpujo6EK3C1hPd2msXbtWjz/+uF588UV16dJFO3bs0KhRozR58mSNGzeuwH3GjBmjxMTEnMfp6emKjY1Vjx49ivzBIDi4XC6tWrVK3bt3V1hYWKDLgZ9oN3ui3eyJdrMn2s1+aDN7ot3syS7t5h1FXZyAhe769esrJCREqampPutTU1PVuHHjAvcZN26cbr31Vt1+++2SpHPOOUcZGRm64447NHbsWDmd+U9Rj4iIUERERL71YWFhQd2A8EV72RPtZk+0mz3RbvZEu9kPbWZPtJs9BXu7+VtbwCZSCw8PV6dOnZScnJyzzuPxKDk52We4eV6ZmZn5gnVISIgkKUCj5AEAAAAAKFRAh5cnJiZq8ODBOvfcc9W5c2dNnz5dGRkZGjp0qCRp0KBBiomJ0ZQpUyRJ1157raZOnaqOHTvmDC8fN26crr322pzwDQAAAABAsAho6O7fv78OHjyopKQkpaSkqEOHDlqxYoUaNWokSdq9e7dPz/Yjjzwih8OhRx55RHv37lWDBg107bXX6rHHHgvUWwAAAAAAoFABn0ht5MiRGjlyZIHPrV271udxaGioxo8fr/Hjx1dAZQAAAAAAlE3AzukGAAAAAKCyI3QDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYJGgCN0zZsxQXFycIiMj1aVLF23atKnQbS+99FI5HI58y9VXX12BFQMAAAAAULyAh+7FixcrMTFR48eP1+bNm9W+fXv17NlTBw4cKHD7pUuXav/+/TnL999/r5CQEN14440VXDkAAAAAAEULeOieOnWqhg8frqFDh6pt27aaNWuWoqKiNGfOnAK3r1u3rho3bpyzrFq1SlFRUYRuAAAAAEDQCWjozs7O1tdff61u3brlrHM6nerWrZs2bNjg1zFmz56tm2++WdWrV7eqTAAAAAAASiU0kC9+6NAhud1uNWrUyGd9o0aN9NNPPxW7/6ZNm/T9999r9uzZhW6TlZWlrKysnMfp6emSJJfLJZfLVcrKUVG8bURb2QvtZk+0mz3RbvZEu9kPbWZPtJs92aXd/K0voKG7rGbPnq1zzjlHnTt3LnSbKVOmaOLEifnWf/TRR4qKirKyPJSjVatWBboElALtZk+0mz3RbvZEu9kPbWZPtJs9BXu7ZWZm+rVdQEN3/fr1FRISotTUVJ/1qampaty4cZH7ZmRkaNGiRZo0aVKR240ZM0aJiYk5j9PT0xUbG6sePXooOjq69MWjQrhcLq1atUrdu3dXWFhYoMuBn2g3e6Ld7Il2syfazX5oM3ui3ezJLu3mHUVdnICG7vDwcHXq1EnJycnq3bu3JMnj8Sg5OVkjR44sct8333xTWVlZuuWWW4rcLiIiQhEREfnWh4WFBXUDwhftZU+0mz3RbvZEu9kT7WY/tJk90W72FOzt5m9tAR9enpiYqMGDB+vcc89V586dNX36dGVkZGjo0KGSpEGDBikmJkZTpkzx2W/27Nnq3bu36tWrF4iyAQAAAAAoVsBDd//+/XXw4EElJSUpJSVFHTp00IoVK3ImV9u9e7ecTt9J1rdv367PPvtMH330USBKBgAAAADALwEP3ZI0cuTIQoeTr127Nt+6Vq1ayTAMi6sCAAAAAKBsAnqdbgAAAAAAKjNCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGCRgIfuGTNmKC4uTpGRkerSpYs2bdpU5PZHjhzRiBEj1KRJE0VEROjMM8/U8uXLK6haAAAAAAD8FxrIF1+8eLESExM1a9YsdenSRdOnT1fPnj21fft2NWzYMN/22dnZ6t69uxo2bKglS5YoJiZGu3btUu3atSu+eAAAAAAAihHQ0D116lQNHz5cQ4cOlSTNmjVLH3zwgebMmaPRo0fn237OnDk6fPiw1q9fr7CwMElSXFxcRZYMAAAAAIDfAja8PDs7W19//bW6deuWW4zTqW7dumnDhg0F7vPuu+8qPj5eI0aMUKNGjXT22Wfr8ccfl9vtrqiyAQAAAADwW8B6ug8dOiS3261GjRr5rG/UqJF++umnAvf57bfftHr1ag0cOFDLly/Xjh07dNddd8nlcmn8+PEF7pOVlaWsrKycx+np6ZIkl8sll8tVTu8GVvG2EW1lL7SbPdFu9kS72RPtZj+0mT3RbvZkl3bztz6HYRiGxbUUaN++fYqJidH69esVHx+fs/7BBx/UunXrtHHjxnz7nHnmmTpx4oR27typkJAQSeYQ9aeeekr79+8v8HUmTJigiRMn5lu/YMECRUVFldO7AQAAAABUJZmZmUpISFBaWpqio6ML3a5UPd0nT57U2rVr9euvvyohIUE1a9bUvn37FB0drRo1avh1jPr16yskJESpqak+61NTU9W4ceMC92nSpInCwsJyArcktWnTRikpKcrOzlZ4eHi+fcaMGaPExMScx+np6YqNjVWPHj2K/MEgOLhcLq1atUrdu3fPOY8fwY92syfazZ5oN3ui3eyHNrMn2s2e7NJu3lHUxSlx6N61a5euvPJK7d69W1lZWerevbtq1qypJ598UllZWZo1a5ZfxwkPD1enTp2UnJys3r17S5I8Ho+Sk5M1cuTIAve54IILtGDBAnk8Hjmd5unoP//8s5o0aVJg4JakiIgIRURE5FsfFhYW1A0IX7SXPdFu9kS72RPtZk+0m/3QZvZEu9lTsLebv7WVeCK1UaNG6dxzz9Vff/2latWq5azv06ePkpOTS3SsxMREvfzyy3rllVe0bds23XnnncrIyMiZzXzQoEEaM2ZMzvZ33nmnDh8+rFGjRunnn3/WBx98oMcff1wjRowo6dsAAAAAAMByJe7p/vTTT7V+/fp8PctxcXHau3dviY7Vv39/HTx4UElJSUpJSVGHDh20YsWKnMnVdu/endOjLUmxsbFauXKl7rvvPrVr104xMTEaNWqUHnrooZK+DQAAAAAALFfi0O3xeAq8RNeePXtUs2bNEhcwcuTIQoeTr127Nt+6+Ph4ffHFFyV+HQAAAAAAKlqJh5f36NFD06dPz3nscDh07NgxjR8/Xr169SrP2gAAAAAAsLUS93Q/88wz6tmzp9q2basTJ04oISFBv/zyi+rXr6+FCxdaUSMAAAAAALZU4tDdrFkzffvtt1q0aJG+++47HTt2TMOGDdPAgQN9JlYDAAAAAKCqK9V1ukNDQ3XLLbeUdy0AAAAAAFQqJQ7dr776apHPDxo0qNTFAAAAAABQmZQ4dI8aNcrnscvlUmZmpsLDwxUVFUXoBgAAAADgbyWevfyvv/7yWY4dO6bt27frwgsvZCI1AAAAAADyKHHoLsg//vEPPfHEE/l6wQEAAAAAqMrKJXRL5uRq+/btK6/DAQAAAABgeyU+p/vdd9/1eWwYhvbv368XXnhBF1xwQbkVBgAAAACA3ZU4dPfu3dvnscPhUIMGDXT55ZfrmWeeKa+6AAAAAACwvRKHbo/HY0UdAAAAAABUOuV2TjcAAAAAAPDlV093YmKi3wecOnVqqYsBAAAAAKAy8St0f/PNN34dzOFwlKkYAAAAAAAqE79C95o1a6yuAwAAAACASodzugEAAAAAsEiJZy+XpK+++kpvvPGGdu/erezsbJ/nli5dWi6FAQAAAABgdyXu6V60aJG6du2qbdu26e2335bL5dIPP/yg1atXq1atWlbUCAAAAACALZU4dD/++OOaNm2a3nvvPYWHh+vZZ5/VTz/9pJtuuknNmze3okYAAAAAAGypxKH7119/1dVXXy1JCg8PV0ZGhhwOh+677z699NJL5V4gAAAAAAB2VeLQXadOHR09elSSFBMTo++//16SdOTIEWVmZpZvdQAAAAAA2Jjfodsbri+++GKtWrVKknTjjTdq1KhRGj58uAYMGKArrrjCmioBAAAAALAhv2cvb9eunc477zz17t1bN954oyRp7NixCgsL0/r169W3b1898sgjlhUKAAAAAIDd+B26161bp7lz52rKlCl67LHH1LdvX91+++0aPXq0lfUBAAAAAGBbfg8vv+iiizRnzhzt379fzz//vH7//XddcsklOvPMM/Xkk08qJSXFyjoBAAAAALCdEk+kVr16dQ0dOlTr1q3Tzz//rBtvvFEzZsxQ8+bNdd1111lRIwAAAAAAtlTi0J3XGWecoYcffliPPPKIatasqQ8++KC86gIAAAAAwPb8Pqf7VJ988onmzJmjt956S06nUzfddJOGDRtWnrUBAAAAAGBrJQrd+/bt07x58zRv3jzt2LFDXbt21XPPPaebbrpJ1atXt6pGAAAAAABsye/QfdVVV+njjz9W/fr1NWjQIN12221q1aqVlbUBAAAAAGBrfofusLAwLVmyRNdcc41CQkKsrAkAAAAAgErB79D97rvvWlkHAAAAAACVTplmLwcAAAAAAIUjdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCdzCZMEGaPLng5yZPNp8HAAAAANgGoTuYhIRISUn5g/fkyeb6kJDA1AUAAAAAKJXQQBeAPMaNM2+TkszbRx6RHn3UfDxpUu7zAAAAAABbIHQHm3HjpKNHzaA9frxkGARuAAAAALAphpcHI2/ANgwpLIzADQAAAAA2RegORtOn5953uQqfXA0AAAAAENQI3cHGO2laQoL5uEaNgidXAwAAAAAEPUJ3MPEG7kmTpLlzpbp1pWPHpEGDCN4AAAAAYEOE7mDidudOmhYeLt10k7ne6TTXu92BrQ8AAAAAUCLMXh5MJkzwfZyQIM2aJb31lpSaKlWrFpCyAAAAAAClQ093MLvgAql5c/MSYu+/H+hqAAAAAAAlROgOZk5n7oRqr78e2FoAAAAAACUWFKF7xowZiouLU2RkpLp06aJNmzYVuu28efPkcDh8lsjIyAqstoINHGjeLl8u/fVXYGsBAAAAAJRIwEP34sWLlZiYqPHjx2vz5s1q3769evbsqQMHDhS6T3R0tPbv35+z7Nq1qwIrrmBnny2dc455ve4lSwJdDQAAAACgBAIeuqdOnarhw4dr6NChatu2rWbNmqWoqCjNmTOn0H0cDocaN26cszRq1KgCKw4Ab283Q8wBAAAAwFYCGrqzs7P19ddfq1u3bjnrnE6nunXrpg0bNhS637Fjx9SiRQvFxsbq+uuv1w8//FAR5QbOgAHm7SefSH/8EdhaAAAAAAB+C+glww4dOiS3252vp7pRo0b66aefCtynVatWmjNnjtq1a6e0tDQ9/fTT6tq1q3744Qc1a9Ys3/ZZWVnKysrKeZyeni5Jcrlccrlc5fhuLNSkiUIuvFDOzz6T+/XX5bn//kBXVGG8bWSbtoIk2s2uaDd7ot3siXazH9rMnmg3e7JLu/lbn8MwDMPiWgq1b98+xcTEaP369YqPj89Z/+CDD2rdunXauHFjscdwuVxq06aNBgwYoMmTJ+d7fsKECZo4cWK+9QsWLFBUVFTZ3kAFarFypTrMnKm0uDitnT490OUAAAAAQJWWmZmphIQEpaWlKTo6utDtAtrTXb9+fYWEhCg1NdVnfWpqqho3buzXMcLCwtSxY0ft2LGjwOfHjBmjxMTEnMfp6emKjY1Vjx49ivzBBJ3zz5fxv/+p1u+/q1eLFtJZZwW6ogrhcrm0atUqde/eXWFhYYEuB36i3eyJdrMn2s2eaDf7oc3siXazJ7u0m3cUdXECGrrDw8PVqVMnJScnq3fv3pIkj8ej5ORkjRw50q9juN1ubd26Vb169Srw+YiICEVERORbHxYWFtQNmE+jRtJVV0nvvquwN9+UOnQIdEUVynbtBUm0m13RbvZEu9kT7WY/tJk90W72FOzt5m9tAZ+9PDExUS+//LJeeeUVbdu2TXfeeacyMjI0dOhQSdKgQYM0ZsyYnO0nTZqkjz76SL/99ps2b96sW265Rbt27dLtt98eqLdQcRISzNsFC6TAnRUAAAAAAPBTQHu6Jal///46ePCgkpKSlJKSog4dOmjFihU5k6vt3r1bTmfudwN//fWXhg8frpSUFNWpU0edOnXS+vXr1bZt20C9hYpz7bVSjRrS779L69dLF1wQ6IoAAAAAAEUIeOiWpJEjRxY6nHzt2rU+j6dNm6Zp06ZVQFVBKCpKuuEG6dVXzd5uQjcAAAAABLWADy9HCXmHmL/xhhTkU+gDAAAAQFVH6LabK66QGjaUDh2SPvoo0NUAAAAAAIpA6Lab0FDp5pvN+6+/HthaAAAAAABFInTb0cCB5u0770jHjgW2FgAAAABAoQjddnTeeVLLllJmphm8AQAAAABBidBtRw5Hbm83Q8wBAAAAIGgRuu3KO4v5Rx9JBw8GthYAAAAAQIEI3XbVqpXUqZPkdpuXDwMAAAAABB1Ct50xxBwAAAAAghqh285uvllyOqUNG6Tffgt0NQAAAACAUxC67axJE+nyy837CxcGthYAAAAAQD6EbrvzTqj2+uuSYQS2FgAAAACAD0K33d1wgxQRIW3bJm3ZEuhqAAAAAAB5ELrtrlYt6dprzfsLFgS2FgAAAACAD0J3ZeAdYr5woXkJMQAAAABAUCB0Vwa9ekm1a0t790qffBLoagAAAAAAfyN0VwYREVK/fuZ9rtkNAAAAAEGD0F1ZDBxo3i5ZImVlBbYWAAAAAIAkQnflcfHFUkyMlJYmLV8e6GoAAAAAACJ0Vx5OpzRggHmfIeYAAAAAEBQI3ZWJd4j5+++bPd4AAAAAgIAidFcm7dtLbdqY53QvXRroagAAAACgyiN0VyYOR25vN0PMAQAAACDgCN2VTUKCebt6tbRvX2BrAQAAAIAqjtBd2Zx2mtS1q2QY0uLFga4GAAAAAKo0Qndl5O3tZog5AAAAAAQUobsyuukmKSRE+vprafv2QFcDAAAAAFUWobsyatBA6tnTvL9gQWBrAQAAAIAqjNBdWeUdYm4Yga0FAAAAAKooQndldf31UlSU9Ouv0qZNga4GAAAAAKokQndlVaOG1Lu3eZ8J1QAAAAAgIAjdldnAgebt4sXSyZOBrQUAAAAAqiBCd2XWvbtUv7504ICUnBzoagAAAACgyiF0V2ZhYeblwySGmAMAAABAABC6KzvvEPO335YyMwNbCwAAAABUMYTuyi4+XoqLk44dk957L9DVAAAAAECVQuiu7BwO32t2AwAAAAAqDKG7KvAOMf/wQ+nPPwNbCwAAAABUIYTuqqBtW6lDB/OyYUuWBLoaAAAAAKgyCN1VBUPMAQAAAKDCEbqrigEDzPO7P/1U2rUr0NUAAAAAQJVA6K4qmjWTLrnEvL9oUWBrAQAAAIAqgtBdlXgnVGOIOQAAAABUCEJ3VdK3rxQeLm3dai4AAAAAAEsRuquSOnWkXr3M+/R2AwAAAIDlCN1VjXeI+cKFkscT2FoAAAAAoJIjdFc1V18t1awp7d4tff55oKsBAAAAgEqN0F3VVKtmntstMcQcAAAAACxG6K6KvEPM33xTys4ObC0AAAAAUIkRuquiyy6TGjeWDh+WVq4MdDUAAAAAUGkRuquikBDp5pvN+wwxBwAAAADLELqrKu8Q83fflY4eDWwtAAAAAFBJEbqrqk6dpDPPlI4fl5YtC3Q1AAAAAFApBUXonjFjhuLi4hQZGakuXbpo06ZNfu23aNEiORwO9e7d29oCKyOHQ0pIMO8zxBwAAAAALBHw0L148WIlJiZq/Pjx2rx5s9q3b6+ePXvqwIEDRe73+++/64EHHtBFF11UQZVWQt4h5qtWSampga0FAAAAACqhgIfuqVOnavjw4Ro6dKjatm2rWbNmKSoqSnPmzCl0H7fbrYEDB2rixIk6/fTTK7DaSuaMM6TOnSWPR3rjjUBXAwAAAACVTkBDd3Z2tr7++mt169YtZ53T6VS3bt20YcOGQvebNGmSGjZsqGHDhlVEmZWbt7ebIeYAAAAAUO5CA/nihw4dktvtVqNGjXzWN2rUSD/99FOB+3z22WeaPXu2tmzZ4tdrZGVlKSsrK+dxenq6JMnlcsnlcpWu8MqkTx+F3nefHBs3yrVtm9n7HUS8bURb2QvtZk+0mz3RbvZEu9kPbWZPtJs92aXd/K0voKG7pI4ePapbb71VL7/8surXr+/XPlOmTNHEiRPzrf/oo48UFRVV3iXaUny7dmq4ZYt2TJ6sn/v3D3Q5BVq1alWgS0Ap0G72RLvZE+1mT7Sb/dBm9kS72VOwt1tmZqZf2zkMwzAsrqVQ2dnZioqK0pIlS3xmIB88eLCOHDmid955x2f7LVu2qGPHjgoJCclZ5/F4JJnD0rdv366WLVv67FNQT3dsbKwOHTqk6OhoC96V/Thee02hw4bJOPNMndy61ZzZPEi4XC6tWrVK3bt3V1hYWKDLgZ9oN3ui3eyJdrMn2s1+aDN7ot3syS7tlp6ervr16ystLa3IbBnQnu7w8HB16tRJycnJOaHb4/EoOTlZI0eOzLd969attXXrVp91jzzyiI4ePapnn31WsbGx+faJiIhQREREvvVhYWFB3YAVql8/acQIOX7+WWFbt5rX8A4ytJc90W72RLvZE+1mT7Sb/dBm9kS72VOwt5u/tQV8eHliYqIGDx6sc889V507d9b06dOVkZGhoUOHSpIGDRqkmJgYTZkyRZGRkTr77LN99q9du7Yk5VuPEoiOlq67zpzB/PXXgzJ0AwAAAIAdBTx09+/fXwcPHlRSUpJSUlLUoUMHrVixImdytd27d8vpDPiVzSq/gQPN0L1okfTUU1KeIfwAAAAAgNIJeOiWpJEjRxY4nFyS1q5dW+S+8+bNK/+CqqIrr5Tq1JH275fWrpWuuCLQFQEAAACA7dGFDFN4uHTjjeZ9rtkNAAAAAOWC0I1cAweat2+9JZ04EdhaAAAAAKASIHQj14UXSrGxUnq69MEHga4GAAAAAGyP0I1cTqc0YIB5nyHmAAAAAFBmhG748g4x/+AD6a+/AlsLAAAAANgcoRu+2rWTzj5bys6Wli4NdDUAAAAAYGuEbuTn7e1miDkAAAAAlAmhG/l5z+teu1bauzegpQAAAACAnRG6kV+LFuZM5oYhLVwY6GoAAAAAwLYI3SiYd4j5ggWBrQMAAAAAbIzQjYL16yeFhkrffCNt2xboagAAAADAlgjdKFj9+tKVV5r3mVANAAAAAEqF0I3C5R1ibhiBrQUAAAAAbIjQjcJdd51Uvbq0c6f0xReBrgYAAAAAbIfQjcJFRUl9+pj3GWIOAAAAACVG6EbRvEPMFy+WXK7A1gIAAAAANkPoRtG6dZMaNJAOHZI+/jjQ1QAAAACArRC6UbTQUKl/f/M+Q8wBAAAAoEQI3Sied4j5smVSRkZASwEAAAAAOyF0o3hdukgtW5qB+913A10NAAAAANgGoRvFczikhATzPkPMAQAAAMBvhG74xxu6V640J1UDAAAAABSL0A3/tG4t/fOf0smT0htvBLoaAAAAALAFQjf8551QbcGCwNYBAAAAADZB6Ib/+vc3z+/+/HPp998DXQ0AAAAABD1CN/wXEyNddpl5n95uAAAAACgWoRsl4x1i/vrrkmEEthYAAAAACHKEbpRM375SRIT044/Sd98FuhoAAAAACGqEbpRMrVrS1Veb97lmNwAAAAAUidCNkvMOMV+4UPJ4AlsLAAAAAAQxQjdKrlcvs8d7zx7p008DXQ0AAAAABC1CN0ouMtI8t1tiiDkAAAAAFIHQjdLxDjF/800pKyuwtQAAAABAkCJ0o3QuuURq2lQ6ckRasSLQ1QAAAABAUCJ0o3RCQqQBA8z7DDEHAAAAgAIRulF6CQnm7XvvSenpga0FAAAAAIIQoRul17Gj1Lq1dOKEtHRpoKsBAAAAgKBD6EbpTZxontctSQsW+D43ebI0YUKFlwQAAAAAwYTQjdILCZFWrzbvJydLKSnm/cmTpaQk83kAAAAAqMJCA10AbGzcOPM2KUnyeKRFi6SjR83HkyblPg8AAAAAVRShG2Uzbpy0caP0wQdSYqJkGARuAAAAAPgbw8tRdnPmmLeGYd4uWyY995x06FDASgIAAACAYEDoRtn997/mrfPvf06bN0ujRpmTrPXpI73zjuRyBa4+AAAAAAgQQjfKxjtp2qRJktstjR5trm/a1Azay5ZJvXtLMTHSvfdKW7YErlYAAAAAqGCEbpRe3sDtPYd7yhTz8b590ogR0gMPSI0bSwcPSs8+a17bu317ado06cCBwNYPAAAAABYjdKP03O6CJ00bN85cX7++9NRT0h9/mBOt3XijFB4uffedOelaTIx03XXS0qVSdnZg3gMAAAAAWIjZy1F6EyYU/lzeIB4aKvXqZS6HD0uLF0vz5kmbNknvvWcudetKCQnSkCHSP/8pORwWFw8AAAAA1qOnGxWrbl3pzjvNy4z9+KP00EPm+d+HD0svvCCde650zjnS009L+/cHuloAAAAAKBNCNwKnTRvpiSek3bulFSukm2+WIiOlH36Q/vMfqVkzhVx/vZp+/rl04kSgqwUAAACAEiN0I/BCQqSePaWFC83e7f/+V4qPlzweOT/8UOc99ZRCmzeX7rrLHJLuvR44AAAAAAQ5QjeCS+3a0h13SOvXS9u3y/3QQzper54cR45IM2dKXbpIbdtKTz4p7d0b6GoBAAAAoEiEbgSvM8+UZ/JkffTSSzr54YfSwIFStWrSTz+Z1wNv3ly68kpp0SLp+PFAVwsAAAAA+RC6EfxCQmRccYU0f76UkiL973/SRRdJHo+0cqU0YIDUpIn0r3+ZPeQMPwcAAAAQJIIidM+YMUNxcXGKjIxUly5dtGnTpkK3Xbp0qc4991zVrl1b1atXV4cOHfTaa69VYLUIqOhoadgw6ZNPpF9+MS9N1qKFlJYmvfSSdMEFUqtW0uOPm9cHBwAAAIAACnjoXrx4sRITEzV+/Hht3rxZ7du3V8+ePXXgwIECt69bt67Gjh2rDRs26LvvvtPQoUM1dOhQrVy5soIrR8CdcYY0aZL022/S6tXSoEFSVJQZxseONcN49+7S669LmZnmdcUnTy74WJMnF33dcQAAAAAohYCH7qlTp2r48OEaOnSo2rZtq1mzZikqKkpz5swpcPtLL71Uffr0UZs2bdSyZUuNGjVK7dq102effVbBlSNoOJ3SZZdJr7xiDj+fO1e69FJzmPnHH0u33CI1biy9956UlGQG9bwmTzbXh4QEpHwAAAAAlVdoIF88OztbX3/9tcaMGZOzzul0qlu3btqwYUOx+xuGodWrV2v79u168sknC9wmKytLWVlZOY/T09MlSS6XSy6Xq4zvAFbztpHfbRUZaU64NnCgtHOnnPPnyzl/vhw7d0qbN5vbjB8vz5o1cs+cKeeiRQqZOFHu8ePlGT1a4t9EuShxuyEo0G72RLvZE+1mP7SZPdFu9mSXdvO3PodhBG7WqX379ikmJkbr169XfHx8zvoHH3xQ69at08aNGwvcLy0tTTExMcrKylJISIhefPFF3XbbbQVuO2HCBE2cODHf+gULFigqKqp83giCm8ejetu2KXb1asV8/rlCT5zwefrPNm3046BBOnzmmfR2AwAAAPBLZmamEhISlJaWpujo6EK3s2Xo9ng8+u2333Ts2DElJydr8uTJWrZsmS699NJ82xbU0x0bG6tDhw4V+YNBcHC5XFq1apW6d++usLCwsh8wI0OOt99WyLBhcpzyT9+oX1/GlVfKc/XVMrp3NydtQ6mUe7uhQtBu9kS72RPtZj+0mT3RbvZkl3ZLT09X/fr1iw3dAR1eXr9+fYWEhCg1NdVnfWpqqho3blzofk6nU2eccYYkqUOHDtq2bZumTJlSYOiOiIhQREREvvVhYWFB3YDwVW7tVbu2tGePeb53eLiUnS2dc470xx9yHDokx9/D0RUWZp4Xfu215hIXV/bXroL4nNkT7WZPtJs90W72Q5vZE+1mT8Hebv7WFtCJ1MLDw9WpUyclJyfnrPN4PEpOTvbp+S6Ox+Px6c0GCuWdNG3SJCkry7zdulUaNUpas0ZKTJT+8Q/z3O5Vq6R77pFOO80M5g8/LG3YILndgX4XAAAAAGwioD3dkpSYmKjBgwfr3HPPVefOnTV9+nRlZGRo6NChkqRBgwYpJiZGU6ZMkSRNmTJF5557rlq2bKmsrCwtX75cr732mmbOnBnItwE7yBu4x40z13lvvbOXP/OMuWzfbs52/t570uefS99/by5TpkgNGkhXX232gPfoIdWoEbj3BAAAACCoBTx09+/fXwcPHlRSUpJSUlLUoUMHrVixQo0aNZIk7d69W05nbod8RkaG7rrrLu3Zs0fVqlVT69atNX/+fPXv3z9QbwF24Xb7Bm4v7+O8PditWpnLAw9Ihw9LH35oBvAVK6SDB6V588wlPNy8XJl3GHrz5hX1bgAAAADYQMBDtySNHDlSI0eOLPC5tWvX+jx+9NFH9eijj1ZAVah0Jkwo/LlTg3hedevmXobM5ZI+/TS3F/zXX6WVK81l5EipXbvcAH7eeeY1xAEAAABUWSQCoCTCwqTLL5emTZN++UX68UfpySelCy80A/Z330mPPSadf77UtKk0bJi0bJmUkRHoygEAAAAEAKEbKC2HQ2rTRnrwQbP3OzVVevVV6cYbpZo1zcdz5kh9+kj16km9ekkzZ5qzpwMAAACoEgjdQHmpX1+69VbpjTekQ4d8Zz/PyjLPC7/rLik2VurY0Zy87csvJY8n0JUDAAAAsAihG7BCeLjUrZv07LPmed/emc+7djV7yLdsMWdT79xZatZMGj7cPEc8MzP3GBMmmNsUZPLkos9RBwAAABAUCN2A1RwO6ayzpNGjzcuPpaaaM5/37Wtebmz/ful//5Ouu84chn7ttdJLL5nngScl5Q/e3kufhYQE5O0AAAAA8F9QzF4OVCkNGkiDB5tLVpa0bl3ubOi7dknvv28ukjkZW1KStG+f9OKL0qOP5r/WOAAAAICgRU83EEgREVKPHtLzz0s7d/rOfu5wmGFbkmbNMmdHT0qSunQxw/hXX0nHjwe2fgAAAABFoqcbCBYOh3TOOeby8MPmMPTly80e8Lffzt1u40ZzkcwgfuaZUvv2vkvTpubxAAAAAAQUoRsIVo0aSUOHmpcYe/tt8xrhLpc5GVu1atK335qzpP/0k7ksXpy7b716Zvhu1y43iLdta/asAwAAAKgwhG4gmHknTfOew5338apV5iRs335rLt99Z95u3y79+ae0erW5eIWGSq1b54ZwbyBv3Dhw7w8AAACo5AjdQLA6NXBLubdJSbmPmzaVrroqd78TJ6QffsgN497lyBHz0mXffy+9/nru9g0b5h+e3rq12bMOAAAAoEwI3UCwcrsLnqXc+9jtLni/yEipUydz8TIMc5j6qUH8l1+kAwfMXvNVq3K3Dwszh6OfGsbr1y+83gkTzMuYFTSr+uTJZr1cWxwAAABVDKEbCFZFBdSSXi7M4ZBiY83lmmty12dkmD3feYenf/edlJ6eG8zzato0/7niZ55pDl0PCcntgR89OnefvD32AAAAQBVD6AaqsurVzUuQdemSu84wpN9/z98r/ttv5iXM9u2TPvwwd/vISOmss8wA3quXlJQkZ0aGFB8v52OPSRMncl1xAAAAVFmEbgC+HA7ptNPMpXfv3PVHj0pbt/oG8a1bzd7yr782l7+FPPmkrpPkkMxLoJ04If3vf7nHjY3lnHEAAABUCYRuAP6pWdO8XFnXrrnrPB7p11/zz6C+a5dyrhK+dau55BUSYgZvbwg/dWncmOuMAwAAoFIgdAMoPadT+sc/zKVfP3Pd3+dwe0JC5HS7pR49pJYtzeHpO3eaQ9ezs83b33+X1qzJf9zISCkuzgzgp5+eP5TXrl1hbxEAAAAoC0I3gPLzd+B2jx+v9zt21DXffKMQ7zndL75obuPxmNcX37nTd/GG8j17zOHoP/1kLgWpXTs3gJ8ayuPizNBeHGZbBwAAQAUgdBfC7XbL5XIFuowqz+VyKTQ0VCdOnJC7sEtk5REWFqaQkJAKqAz55Jml3DN6tLR8uTxjx5rtkfe64k6nFBNjLhdemP842dnS7t35Q7l3OXjQvOb4N9+YS0GaNMnfO+4N582amWH71LoKeB8AAABAWRG6T2EYhlJSUnTkyJFAlwKZ7dG4cWP98ccfcvh5jm/t2rXVuHFjv7dHOcl7XfG8X1gVd13xU4WHS2ecYS4FOXas8EC+c6f5/P795rJ+ff79Q0Ol5s3NAN6pkxmwv/tOGjVKWrJEevZZZlsHAABAuSF0n8IbuBs2bKioqCiCW4B5PB4dO3ZMNWrUkNPpLHJbwzCUmZmpAwcOSJKaNGlSESXCqzyvK16UGjXMGdHPOSf/c4YhHTpUeCDftcv8QuC338zFa8kSc/F68knp1VfNCd3yLk2a+D5u2NAM8QAAAEAh+GsxD7fbnRO469WrF+hyIDN0Z2dnKzIystjQLUnVqlWTJB04cEANGzZkqHlV43BIDRqYS+fO+Z93u83rjHvPH/cu8+ebgd0rI0PascNc/Hm94sJ548ZSrVplm5Gdc9ABAABsidCdh/cc7qioqABXgrLwtp/L5SJ0w5f3UmWxsdIll5jrJk82A3d4uHk++dix0uDBUkqKOUQ9JSX/sn+/dOCAOSncgQPm8t13Rb92ZKR/4bxxY7OWgmrnHHQAAADbIXQXgCHl9kb7wW95A+u4cbmPIyKKHxLvdptD2U8N4wWF9LQ0c0Z272XSilO3bsHhvG9fs77UVPN25kyzd5tz0AEAAIIWoRtA1XRq4JZybwvqUT5VSIjUqJG5tG9f9GtlZppBuaAe81PXuVzS4cPm8uOPBR9vxgxzkcwe9FdflZYvl+rVk+rXN5e89/M+rlu3Ys5DZzg8AACAJEK3Zdxu6dNPzb+pmzSRLrrI/PvTahs2bNCFF16oK6+8Uh988IH1LwjYVd7Z1vMq6Wzr/oiKyr10WVEMQ/rrr6KHtaekSD/8kLvPiRP+nX+eV+3aBYZzZ506arFvnxzZ2WbPelmCOsPhAQAAJBG6LbF0qXn1oT17ctc1a2ZeieiGG6x97dmzZ+vuu+/W7NmztW/fPjVt2tTaFyxEdna2wgs6LxUIFhU123pJOBxmwK1bVzrrrIK38YZW7znod98t3XST9Oef5nB375L3sff+4cPmMY4cMZdTgnqIpA6S9OKL+V+3Tp3Ce9IL6lUfM8bcL2/wLmh0AQAAQCVH6C5nS5dK/fr5ToQsSXv3muuXLLEueB87dkyLFy/WV199pZSUFM2bN08PP/xwzvPvvfeeJk2apK1bt6pGjRq66KKL9Pbbb0uSsrKylJSUpAULFujAgQOKjY3VmDFjNGzYMM2bN0/33nuvz7XLly1bpj59+sj4+41OmDBBy5Yt08iRI/XYY49p165d8ng8WrFihR599FF9//33CgkJUXx8vJ599lm1bNky51h79uzRf/7zH61cuVJZWVlq06aNZsyYoUaNGun0009XcnKyLvFOeiVp+vTpmjZtmnbu3OnXjOZApVHYOegNGvgXYk+eNHvSCwrkhw7Jc+CAUrdtU6OQEDn//NN8zhvU//rLXErSo16njvkFQlKS+SWHx2POKp+VJT39tNnjXqeO723t2uZM7xU9CSLD4QEAgEUI3cUwDPN0TH+43dI99+QP3N7jOBxmD3i3bv79PRkVVbIrDL3xxhtq3bq1WrVqpVtuuUX33nuvxowZI4fDoQ8++EB9+vTR2LFj9eqrryo7O1vLly/P2XfQoEHasGGDnnvuObVv3147d+7UoUOH/H9xSTt27NBbb72lpUuX5swanpGRocTERLVr107Hjh1TUlKS+vTpoy1btsjpdOrYsWO65JJLFBMTo3fffVeNGzfW5s2b5fF4FBcXpyuuuEKvv/66T+ieO3euhgwZQuBG1VLWc9Alc4i495JqBXC7XNq0fLl69eolZ1iYudIb1AvrPS/o8V9/mft6byUzcEvSpk3mUpzoaN8gXtD9wp6vXr3kl2djODwAALAIobsYmZlSjRrlcyzDMIec16rl3/bHjpl/O/pr9uzZuuWWWyRJV155pdLS0rRu3Tpdeumleuyxx3TzzTdr4sSJOdu3/3vyp59//llvvPGGVq1apW7dukmSTj/9dP9f+G/Z2dl69dVX1SDPH/R9+/b12WbOnDlq0KCBfvzxR5199tlasGCBDh48qC+//FJ169aVJJ1xxhk52w8bNkx33nmnnn/+eVWrVk2bN2/W1q1b9c4775S4PsDWKvIc9LyKCeoFyhvUn3lGmj3bPM7Jk+al2tq1M58/ciT31nvf+y1nerq57NpVuppLGtT79zevz26X4fD0zAMAYBuE7kpi+/bt2rRpU85w8dDQUPXv31+zZ8/WpZdeqi1btmj48OEF7rtlyxaFhIT49CaXRosWLXwCtyT98ssvSkpK0saNG3Xo0CF5/u7t2r17t84++2xt2bJFHTt2zAncp+rdu7dGjhypt99+WwkJCZo3b54uu+wyxcXFlalWwHaC8Rz0wniD+qxZZuA+dTj8FVdIzz1X8L7Z2eYl1goL5afeP3XdyZPm4u11L42kpNzw3bSptHq19NVXUs2a5hId7Xtb1H2rZorP2zM/enTuenrmqya+hAGAoEboLkZUlNnj7I9PPpF69Sp+u+XLpYsv9u+1/TV79mydPHnSZ+I0wzAUERGhF154QdWqVSt036KekySn05lz7raXy+XKt131Arrlr732WrVo0UIvv/yymjZtKo/Ho7PPPlvZ2dl+vXZ4eLhuvvlmzZs3T/369dOCBQv07LPPFrkPgCBQ2uHw4eEl71n38p4PVFQoLyq0p6XlP+a+feZSWtWqFR7K/V3nve8d8i/5/CydbrfUsaOcjz0mTZxIz3xVxOkRABDUCN3FcDj8H+Ldo4c5S/nevQWf1+1wmM/36FG+cwSdPHlSr776qp555hn16NHD57nevXtr4cKFateunZKTkzV06NB8+59zzjnyeDxat25dzvDyvBo0aKCjR48qIyMjJ1hv2bKl2Lr+/PNPbd++XS+//LIuuugiSdJnn33ms027du30v//9T4cPHy60t/vWW29V165d9eKLL+rkyZO6weop4AGUXSCGw3v/w65eXYqJKfn+brdZ35QpZsB1uaRbbpGuvlo6etRc0tPz3y9oXVaWeczjx83lwIGyv7+IiPxB/B//UMjEibrO4ZDDMKSuXc3RAo8/bgb+gpbIyMKfCwsr+fnw/iAUlj/DMP+tHTok9ewpbd9u/iy/+EIaMsQcoTFrVnB+CQMAVQyhuxyFhJiXBevXz/ybJW/w9v4NM316+U/K+/777+uvv/7SsGHDVOuUE8b79u2r2bNn66mnntIVV1yhli1b6uabb9bJkye1fPlyPfTQQ4qLi9PgwYN122235UyktmvXLh04cEA33XSTunTpoqioKD388MO65557tHHjRs2bN6/YuurUqaN69erppZdeUpMmTbR7926NzjsMUtKAAQP0+OOPq3fv3poyZYqaNGmib775Rk2bNlV8fLwkqVWrVjr//PP10EMP6bbbbiu2dxxAELDTcHivxx83A/epw+HPPLPkNWdn+xfO/Vl34oR5zKws6eBBczmFw/sLZ/16cyktp7PwQO5vcC9o6d7dvMZ8UpI5omDsWPMXJj3zuVyu3AkJDx707/bvUWM+li83F6/p06V335VatjSX00/PvW3WzGxzAIClCN3l7IYbzMuCFXSd7unTrblc2OzZs9WtW7d8gVsyQ/f//d//qW7dunrzzTc1efJkPfHEE4qOjtbFeca4z5w5Uw8//LDuuusu/fnnn2revHnO5cbq1q2r+fPn6z//+Y9efvllXXHFFZowYYLuuOOOIutyOp1atGiR7rnnHp199tlq1aqVnnvuOV166aU524SHh+ujjz7S/fffr169eunkyZNq27atZsyY4XOsoUOHav369brtttvK8JMCgEKUx+zweYWHm9crr1ev7LW5XLkh/NRwvmCBtGyZPE6nnB6PdMEFUocOZu/6iRO5Pe3FLV4ejzmhXEZG2esuzDPPmItkBtvnnpPmzMntwa9Ro+j7RT0fEVE+PfVl7ZnP2wtdSGAOSU3VRTt2KPSBB8z1eS7LWSLVqpmnY9Svb96uWpV7tQDJvOzf4cPmvASnCg+XTjvNN4h7w/lpp5XsPDcAQKEcxqkn61Zy6enpqlWrltLS0hQdHe3z3IkTJ7Rz506ddtppioyMLNPruN3Sp59K+/dLTZpIF11U8ZedrQw8Ho/S09P13HPPacmSJfruu++K3ac82xGl43K5tPzvS0+F5T0PFUGtSrebHc85/jsAuseP1/sdO+qab75RSGl6jg3D7EUvSUgvaajPu31pw6U/QkNLF9YLuv/CC9Kjj5o/z9Gjzd75p56Shg6Vrryy6B7ownqhi+NwmF/UeAN0Ubfe+3mDsfdLgfBw8/XHjpVuukn69Vfpt998b3//3Zx0sChNmuTvHfeG8gYNrDkVIYhV6f8jbYx2sye7tFtR2TIverotEhIi5enQRSkdO3ZMP/74o2bMmKFHH3000OUAqKzsNhw+T4+rZ/RoaflyecaOVUhhPbRFcTjM4eKRkebl06x0aihMTJRuu82csfTo0dzbkt73XmrOe7m6vNeIL6u8s9lL0ty55uKPqKhCg/PJOnW0efdu/bNHD4U2aWKur1On9N/Qnzpaw/s4IqLgfwsnT5pD8vIG8bz309LMnoP9+6VT5mORZH5BcWoY9962aOE78V9edvyCCwDKiNCNoHb33Xdr0aJFuv766xlaDgBeeSeqy3s1Cauv214WhYXC2rXL/sWG220OiT81jJc2xB87VvCMqP70QOftiS5ieLbhcmn/8uUyLryw8IDqr9KcHhEaKsXFmcsVV5xSnGF+cVFYIN+zx/wZffeduZzK6ZSaN8/fO3766ea/14kT89fEpHoAKjFCN4La3Llz9eyzzyo6OlpOJnsBAJONe+bL5Zz5U4WEmLO5FzG0r0Q8HnMo/MSJ5pByb8/8PfcE58+3vK8W4HBIdeuay3nn5X8+K8scnl7QsPXffjN/dr//bi7Jyfn3r1bNbPc33pCuuUb6+mvzXPQbbpDOOUf6+OOCh/xbdd37otAzD6AcELoBAIC1AnEJubJwOqWpU83AfWrPvBR8wbuiv4SJiJBatTKXUxmGOVN9QYH811/Ny+d5J+/7/ntz8Vq61FyKet2Snp9f1on38p6ykfcKLMHaM8+XBEBQInQDAABr0TNfdTgc5gRsTZpIF16Y//ljx8wQ/ttv5jVW3W7zS44+fQof8u+97n1WlrkcOlQ+tfo78d7ll0tJSXJ+9ZWannmmnAsXSosXm5Pqdesmbd5szokQEZH/NjS0YiecK+vM+wAsQegGAADIy24983ZSo4bUrp30zjvmz9E7dL99+8K/yMjONsN3ac/RP/V8/VJOvBfy7rvyGWzvz6R6TqcZvgsK5N7bop4r6Ta9e5tfSiQlme917FjzmrXjx5f8ygYAyg2hGwAAIC+79czbTWGT6kkF/3zDw3PPMS8PbnepQrzxxhtyeDwyHA45OnQwL4Pnvdxe3tu8l2Lzzg/gHVJfkf7v/8xFMgP5Sy+ZPfS1ahW8REcX/ZwV59TbbTi83epF0CB0AwAAoGIEw9D9kJDcMOmvyZPl8HjkDg1VyMmT5nD4wup0u3OHwhcWzEv7XEn398rKMmed37On9D+36tWLDubFBfdatcwvUPKy23B4u9XLlwRBg9ANAACAimHHoft/Byr3+PF6v2NHXfPNNwop6guCkBDzcnFFXDLOct4QmHfm/VtukdLTzWuwF7YU9Ly3lz4jw1z27y99XZGR+YN5mzZmrR9+aF6+7rPPpLVrzXPpHQ7p8cfNW6ezZLd/33d4PGr+/fdyHDpkXp6vNMfy3nbpIg0aZNb722/SkCHSggXmKIJ77jEfHzxoztBfrZr5byGQ7PYlQSVG6AYAAEDFsNvQ/TzhxDN6tLR8uTxjxyqksDATDAobvl+/fulqdbmKDuXFhfa0NHOYvmT2wJ84Yc5if6oNG8zFa/VqcymjUEkdy3yUAsybZy5ezz1nLj4vHmqG78hI39uC1pXXc5GRuWG/oFEkBY02CRaVuGee0F1JDBkyREeOHNGyZcsCXQoAAEDlkLdn3uXKXR+sPfNWDN8PCzMDe/36pa/L7TYDeVGhfdy43Nns77jDPB/eMMp863G7dSA1VQ3r15dTKrfj6ocfzFtJatjQ/DLh+HHffycnT+bOD1CRwsJ8A3n9+mb7jx9v1nzmmdK330q33pob2Ita8gb7whans+x12+0SfSVA6C5vlfgbGgAAgCrFbj3zwTp8PyREqlPHXAri/RvZOxy+adNy+/m6XS5tXL5cvXr1kjMsrFyOmW/4/siRvj9jb4/+8eO5t3nvF3Zblm3yhn2Xy1zS033r9n5J8PPP5lKewsP9D+iFhfmYGPNSgklJcm7frnpt28q5fr05IWAw9syXAKG7vAXhuRPr1q3Tf/7zH3377beqW7euBg8erEcffVShf89CuWTJEk2cOFE7duxQVFSUOnbsqHfeeUfVq1fX2rVr9eCDD+qHH35QWFiYzjrrLC1YsEAtWrSo8PcBAACAItjtSwKp5LPZB1px9YaEmJPOVa9esXWdPFlw2D9xQvrvf6U5c8zh7idPmpeW69YtN8TnXfKG++KWvEE/O9tc0tLK5e2EvP66LvQ+sHnglgjdxTOM3Os5+iMx0fwHl5Rk3o4eLT3xhPToo9Ijj5jPZ2T4d6yoKHPShjLYu3evevXqpSFDhujVV1/VTz/9pOHDhysyMlITJkzQ/v37NWDAAP3f//2f+vTpo6NHj+rTTz+VYRg6efKkevfureHDh2vhwoXKzs7Wpk2b5ChjTQAAAEBQzGZfEsFcb2ioVKOGueQ1ebIZuE/9kuCf/yx7rd5efX8CeknC/IkTMr74Qg7DkBEWJkcw/RsoJUJ3cTIz8//j9dejj5pLYY+Lc+xYmb8le/HFFxUbG6sXXnhBDodDrVu31r59+/TQQw8pKSlJ+/fv18mTJ3XDDTfk9F6fc845kqTDhw8rLS1N11xzjVq2bClJatOmTZnqAQAAACQF73D4wtitXqu/JLCqV3/yZDk2bDAv0edyme/D5sGb0F3Jbdu2TfHx8T690xdccIGOHTumPXv2qH379rriiit0zjnnqGfPnurRo4f69eunOnXqqG7duhoyZIh69uyp7t27q1u3brrpppvUpEmTAL4jAAAAVAp2Gw5vt3rt9iWBVPJL9NlEOUwzV8lFRZk9ziVdHnnE3D883Lx95JGSH6MCru8YEhKiVatW6cMPP1Tbtm31/PPPq1WrVtq5c6ckae7cudqwYYO6du2qxYsX68wzz9QXX3xheV0AAAAAymDChMKD6rhxwTe5c95L9I0dK0nm7aRJ5vrJkwNcYOkFReieMWOG4uLiFBkZqS5dumjTpk2Fbvvyyy/roosuUp06dVSnTh1169atyO3LzOHIHTbh7zJ1qjmMfNIkKSvLvH30UXN9SY5TDudOt2nTRhs2bJDhna1Q0ueff66aNWuqWbNmf79Fhy644AJNnDhR33zzjcLDw/X222/nbN+xY0eNGTNG69ev19lnn60FCxaUuS4AAAAAyFFUz/ykScHZM++ngA8vX7x4sRITEzVr1ix16dJF06dPV8+ePbV9+3Y1bNgw3/Zr167VgAED1LVrV0VGRurJJ59Ujx499MMPPygmJiYA7+AUAZxgIS0tTVu2bPFZd8cdd2j69Om6++67NXLkSG3fvl3jx49XYmKinE6nNm7cqOTkZPXo0UMNGzbUxo0bdfDgQbVp00Y7d+7USy+9pOuuu05NmzbV9u3b9csvv2jQoEGW1A8AAACgirLb8P0SCHjonjp1qoYPH66hQ4dKkmbNmqUPPvhAc+bM0ei8F0X/2+uvv+7z+H//+5/eeustJScnB0cYDOC5E2vXrlXHjh191g0bNkzLly/Xf/7zH7Vv315169bVsGHD9Mjfw9+jo6P1ySefaPr06UpPT1eLFi30zDPP6KqrrlJqaqp++uknvfLKK/rzzz/VpEkTjRgxQv/6178sew8AAAAAUJkENHRnZ2fr66+/1pgxY3LWOZ1OdevWTRs2bPDrGJmZmXK5XKpbt26Bz2dlZSkrKyvncfrfF4l3uVxy5b223N/rDMOQx+ORx+Mp6dsxeXuzC9r/73MTCnyujObMmaM5c+YU+nxB52F7PB61atVKy5cvL/C5Bg0a6K233irweKX++ZSQd1i8t1384fF4ZBiGXC6XQkJCrCwPhfB+tk79jCG40W72RLvZE+1mP7SZPdFu9mSXdvO3PoeR92TfCrZv3z7FxMRo/fr1io+Pz1n/4IMPat26ddq4cWOxx7jrrru0cuVK/fDDD4qMjMz3/IQJEzRx4sR86xcsWKCoUyYqCw0NVePGjRUbG6tw7wRosJ3s7Gz98ccfSklJ0cmTJwNdDgAAAIBKKDMzUwkJCUpLS1N0dHSh2wV8eHlZPPHEE1q0aJHWrl1bYOCWpDFjxigxMTHncXp6umJjY9WjR498P5gTJ07ojz/+UI0aNQo9HiqWYRg6evSoatas6XPZs6KcOHFC1apV08UXX0w7BojL5dKqVavUvXt3hYWFBboc+Il2syfazZ5oN/uhzeyJdrMnu7SbdxR1cQIauuvXr6+QkBClpqb6rE9NTVXjxo2L3Pfpp5/WE088oY8//ljt2rUrdLuIiAhFRETkWx8WFpavAd1utxwOh5xOp5zOoJjYvcrzDin3tos/nE6nHA5HgW2MikUb2BPtZk+0mz3RbvZDm9kT7WZPwd5u/tYW0GQZHh6uTp06KTk5OWedx+NRcnKyz3DzU/3f//2fJk+erBUrVujcc8+tiFIBAAAAACixgA8vT0xM1ODBg3Xuueeqc+fOmj59ujIyMnJmMx80aJBiYmI0ZcoUSdKTTz6ppKQkLViwQHFxcUpJSZEk1ahRQzVq1AjY+wAAAAAA4FQBD939+/fXwYMHlZSUpJSUFHXo0EErVqxQo0aNJEm7d+/2GVY8c+ZMZWdnq1+/fj7HGT9+vCYUdW23Egjg3HIoB7QfAAAAgGAR8NAtSSNHjtTIkSMLfG7t2rU+j3///XfL6vCOyc/MzFS1atUsex1YKzMzU5L/51gAAAAAgFWCInQHi5CQENWuXVsHDhyQJEVFRfk9Yzas4fF4lJ2drRMnThQ7kZphGMrMzNSBAwdUu3ZtrtENAAAAIOAI3afwzpruDd4ILMMwdPz4cVWrVs3vL0Bq165d7Oz3AAAAAFARCN2ncDgcatKkiRo2bCiXyxXocqo8l8ulTz75RBdffLFfw8XDwsLo4QYAAAAQNAjdhQgJCSG8BYGQkBCdPHlSkZGRnKMNAAAAwHYCep1uAAAAAAAqM0I3AAAAAAAWIXQDAAAAAGCRKndOt2EYkqT09PQAVwJ/uFwuZWZmKj09nXO6bYR2syfazZ5oN3ui3eyHNrMn2s2e7NJu3kzpzZiFqXKh++jRo5Kk2NjYAFcCAAAAALC7o0ePqlatWoU+7zCKi+WVjMfj0b59+1SzZk2/r/uMwElPT1dsbKz++OMPRUdHB7oc+Il2syfazZ5oN3ui3eyHNrMn2s2e7NJuhmHo6NGjatq0qZzOws/crnI93U6nU82aNQt0GSih6OjooP7AoWC0mz3RbvZEu9kT7WY/tJk90W72ZId2K6qH24uJ1AAAAAAAsAihGwAAAAAAixC6EdQiIiI0fvx4RUREBLoUlADtZk+0mz3RbvZEu9kPbWZPtJs9VbZ2q3ITqQEAAAAAUFHo6QYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuBMyUKVN03nnnqWbNmmrYsKF69+6t7du3F7nPvHnz5HA4fJbIyMgKqhiSNGHChHxt0Lp16yL3efPNN9W6dWtFRkbqnHPO0fLlyyuoWnjFxcXlazeHw6ERI0YUuD2ftcD45JNPdO2116pp06ZyOBxatmyZz/OGYSgpKUlNmjRRtWrV1K1bN/3yyy/FHnfGjBmKi4tTZGSkunTpok2bNln0DqqmotrN5XLpoYce0jnnnKPq1auradOmGjRokPbt21fkMUvzfy1KprjP25AhQ/K1wZVXXlnscfm8Wau4divod53D4dBTTz1V6DH5vFnLn7/5T5w4oREjRqhevXqqUaOG+vbtq9TU1CKPW9rfiYFA6EbArFu3TiNGjNAXX3yhVatWyeVyqUePHsrIyChyv+joaO3fvz9n2bVrVwVVDK+zzjrLpw0+++yzQrddv369BgwYoGHDhumbb75R79691bt3b33//fcVWDG+/PJLnzZbtWqVJOnGG28sdB8+axUvIyND7du314wZMwp8/v/+7//03HPPadasWdq4caOqV6+unj176sSJE4Uec/HixUpMTNT48eO1efNmtW/fXj179tSBAwesehtVTlHtlpmZqc2bN2vcuHHavHmzli5dqu3bt+u6664r9rgl+b8WJVfc502SrrzySp82WLhwYZHH5PNmveLaLW977d+/X3PmzJHD4VDfvn2LPC6fN+v48zf/fffdp/fee09vvvmm1q1bp3379umGG24o8ril+Z0YMAYQJA4cOGBIMtatW1foNnPnzjVq1apVcUUhn/Hjxxvt27f3e/ubbrrJuPrqq33WdenSxfjXv/5VzpWhJEaNGmW0bNnS8Hg8BT7PZy3wJBlvv/12zmOPx2M0btzYeOqpp3LWHTlyxIiIiDAWLlxY6HE6d+5sjBgxIuex2+02mjZtakyZMsWSuqu6U9utIJs2bTIkGbt27Sp0m5L+X4uyKajdBg8ebFx//fUlOg6ft4rlz+ft+uuvNy6//PIit+HzVrFO/Zv/yJEjRlhYmPHmm2/mbLNt2zZDkrFhw4YCj1Ha34mBQk83gkZaWpokqW7dukVud+zYMbVo0UKxsbG6/vrr9cMPP1REecjjl19+UdOmTXX66adr4MCB2r17d6HbbtiwQd26dfNZ17NnT23YsMHqMlGI7OxszZ8/X7fddpscDkeh2/FZCy47d+5USkqKz+epVq1a6tKlS6Gfp+zsbH399dc++zidTnXr1o3PYAClpaXJ4XCodu3aRW5Xkv9rYY21a9eqYcOGatWqle688079+eefhW7L5y34pKam6oMPPtCwYcOK3ZbPW8U59W/+r7/+Wi6Xy+ez07p1azVv3rzQz05pficGEqEbQcHj8ejee+/VBRdcoLPPPrvQ7Vq1aqU5c+bonXfe0fz58+XxeNS1a1ft2bOnAqut2rp06aJ58+ZpxYoVmjlzpnbu3KmLLrpIR48eLXD7lJQUNWrUyGddo0aNlJKSUhHlogDLli3TkSNHNGTIkEK34bMWfLyfmZJ8ng4dOiS3281nMIicOHFCDz30kAYMGKDo6OhCtyvp/7Uof1deeaVeffVVJScn68knn9S6det01VVXye12F7g9n7fg88orr6hmzZrFDlPm81ZxCvqbPyUlReHh4fm+iCzqs1Oa34mBFBroAgBJGjFihL7//vtiz5+Jj49XfHx8zuOuXbuqTZs2+u9//6vJkydbXSYkXXXVVTn327Vrpy5duqhFixZ64403/PomGYE3e/ZsXXXVVWratGmh2/BZA8qfy+XSTTfdJMMwNHPmzCK35f/awLv55ptz7p9zzjlq166dWrZsqbVr1+qKK64IYGXw15w5czRw4MBiJwLl81Zx/P2bv7KhpxsBN3LkSL3//vtas2aNmjVrVqJ9w8LC1LFjR+3YscOi6lCc2rVr68wzzyy0DRo3bpxv9snU1FQ1bty4IsrDKXbt2qWPP/5Yt99+e4n247MWeN7PTEk+T/Xr11dISAifwSDgDdy7du3SqlWriuzlLkhx/9fCeqeffrrq169faBvweQsun376qbZv317i33cSnzerFPY3f+PGjZWdna0jR474bF/UZ6c0vxMDidCNgDEMQyNHjtTbb7+t1atX67TTTivxMdxut7Zu3aomTZpYUCH8cezYMf3666+FtkF8fLySk5N91q1atcqnFxUVZ+7cuWrYsKGuvvrqEu3HZy3wTjvtNDVu3Njn85Senq6NGzcW+nkKDw9Xp06dfPbxeDxKTk7mM1iBvIH7l19+0ccff6x69eqV+BjF/V8L6+3Zs0d//vlnoW3A5y24zJ49W506dVL79u1LvC+ft/JV3N/8nTp1UlhYmM9nZ/v27dq9e3ehn53S/E4MqABP5IYq7M477zRq1aplrF271ti/f3/OkpmZmbPNrbfeaowePTrn8cSJE42VK1cav/76q/H1118bN998sxEZGWn88MMPgXgLVdL9999vrF271ti5c6fx+eefG926dTPq169vHDhwwDCM/G32+eefG6GhocbTTz9tbNu2zRg/frwRFhZmbN26NVBvocpyu91G8+bNjYceeijfc3zWgsPRo0eNb775xvjmm28MScbUqVONb775JmeW6yeeeMKoXbu28c477xjfffedcf311xunnXaacfz48ZxjXH755cbzzz+f83jRokVGRESEMW/ePOPHH3807rjjDqN27dpGSkpKhb+/yqqodsvOzjauu+46o1mzZsaWLVt8ft9lZWXlHOPUdivu/1qUXVHtdvToUeOBBx4wNmzYYOzcudP4+OOPjX/+85/GP/7xD+PEiRM5x+DzVvGK+3/SMAwjLS3NiIqKMmbOnFngMfi8VSx//ub/97//bTRv3txYvXq18dVXXxnx8fFGfHy8z3FatWplLF26NOexP78TgwWhGwEjqcBl7ty5OdtccsklxuDBg3Me33vvvUbz5s2N8PBwo1GjRkavXr2MzZs3V3zxVVj//v2NJk2aGOHh4UZMTIzRv39/Y8eOHTnPn9pmhmEYb7zxhnHmmWca4eHhxllnnWV88MEHFVw1DMMwVq5caUgytm/fnu85PmvBYc2aNQX+v+htG4/HY4wbN85o1KiRERERYVxxxRX52rNFixbG+PHjfdY9//zzOe3ZuXNn44svvqigd1Q1FNVuO3fuLPT33Zo1a3KOcWq7Ffd/LcquqHbLzMw0evToYTRo0MAICwszWrRoYQwfPjxfeObzVvGK+3/SMAzjv//9r1GtWjXjyJEjBR6Dz1vF8udv/uPHjxt33XWXUadOHSMqKsro06ePsX///nzHybuPP78Tg4XDMAzDmj50AAAAAACqNs7pBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAADlxuFwaNmyZYEuAwCAoEHoBgCgkhgyZIgcDke+5corrwx0aQAAVFmhgS4AAACUnyuvvFJz5871WRcRERGgagAAAD3dAABUIhEREWrcuLHPUqdOHUnm0O+ZM2fqqquuUrVq1XT66adryZIlPvtv3bpVl19+uapVq6Z69erpjjvu0LFjx3y2mTNnjs466yxFRESoSZMmGjlypM/zhw4dUp8+fRQVFaV//OMfevfdd6190wAABDFCNwAAVci4cePUt29fffvttxo4cKBuvvlmbdu2TZKUkZGhnj17qk6dOvryyy/15ptv6uOPP/YJ1TNnztSIESN0xx13aOvWrXr33Xd1xhln+LzGxIkTddNNN+m7775Tr169NHDgQB0+fLhC3ycAAMHCYRiGEegiAABA2Q0ZMkTz589XZGSkz/qHH35YDz/8sBwOh/79739r5syZOc+df/75+uc//6kXX3xRL7/8sh566CH98ccfql69uiRp+fLluvbaa7Vv3z41atRIMTExGjp0qB599NECa3A4HHrkkUc0efJkSWaQr1Gjhj788EPOLQcAVEmc0w0AQCVy2WWX+YRqSapbt27O/fj4eJ/n4uPjtWXLFknStm3b1L59+5zALUkXXHCBPB6Ptm/fLofDoX379umKK64osoZ27drl3K9evbqio6N14MCB0r4lAABsjdANAEAlUr169XzDvctLtWrV/NouLCzM57HD4ZDH47GiJAAAgh7ndAMAUIV88cUX+R63adNGktSmTRt9++23ysjIyHn+888/l9PpVKtWrVSzZk3FxcUpOTm5QmsGAMDO6OkGAKASycrKUkpKis+60NBQ1a9fX5L05ptv6txzz9WFF16o119/XZs2bdLs2bMlSQMHDtT48eM1ePBgTZgwQQcPHtTdd9+tW2+9VY0aNZIkTZgwQf/+97/VsGFDXXXVVTp69Kg+//xz3X333RX7RgEAsAlCNwAAlciKFSvUpEkTn3WtWrXSTz/9JMmcWXzRokW666671KRJEy1cuFBt27aVJEVFRWnlypUaNWqUzjvvPEVFRalv376aOnVqzrEGDx6sEydOaNq0aXrggQdUv3599evXr+LeIAAANsPs5QAAVBEOh0Nvv/22evfuHehSAACoMjinGwAAAAAAixC6AQAAAACwCOd0AwBQRXBGGQAAFY+ebgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAs8v+VWGsXIVoQegAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Captured from the experiment (batch size : 128, epochs : 20)\n",
    "\"\"\"\n",
    "\n",
    "accuracy = [\n",
    "    0.8579, \n",
    "\t0.8986, \n",
    "\t0.9056, \n",
    "\t0.9116, \n",
    "\t0.9155, \n",
    "\t0.9184, \n",
    "\t0.9204, \n",
    "\t0.9220, \n",
    "\t0.9235, \n",
    "\t0.9244,\n",
    "    0.9252, \n",
    "\t0.9260, \n",
    "\t0.9249, \n",
    "\t0.9269, \n",
    "\t0.9279, \n",
    "\t0.9285, \n",
    "\t0.9298, \n",
    "\t0.9300, \n",
    "\t0.9305, \n",
    "\t0.9311\n",
    "]\n",
    "\n",
    "loss = [\n",
    "    0.5193, \n",
    "\t0.3251, \n",
    "\t0.2994, \n",
    "\t0.2787, \n",
    "\t0.2661, \n",
    "\t0.2572, \n",
    "\t0.2507, \n",
    "\t0.2454, \n",
    "\t0.2410, \n",
    "\t0.2374,\n",
    "    0.2350, \n",
    "\t0.2316, \n",
    "\t0.2371, \n",
    "\t0.2278, \n",
    "\t0.2244, \n",
    "\t0.2224, \n",
    "\t0.2184, \n",
    "\t0.2180, \n",
    "\t0.2157, \n",
    "\t0.2133\n",
    "]\n",
    "\n",
    "epochs = list(range(1, len(accuracy) + 1))\n",
    "\n",
    "# <Plotting>\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(epochs, accuracy, label='Accuracy', color='blue', marker='o')\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(epochs, loss, label='Loss', color='red', marker='x')\n",
    "\n",
    "plt.title('Model Training Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5385235,
     "sourceId": 8948890,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5416828,
     "sourceId": 8993126,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5416980,
     "sourceId": 8993346,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5482927,
     "sourceId": 9086944,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5483938,
     "sourceId": 9088327,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5484469,
     "sourceId": 9089052,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5484754,
     "sourceId": 9089444,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5506630,
     "sourceId": 9121936,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
